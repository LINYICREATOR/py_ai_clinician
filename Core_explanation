#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Re-implmentation of AI Clinician Matlab Code in Python 
# Author: KyungJoong Kim (GIST, South Korea)
# Date: 2020 June 2 
# 
# This code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE


# Note 
#
# K-Means in scikit-learn will produce differnt outcome with Matlab's original K-means 
# The random number generator will produce different random number sequence 
# 
# Todo: 
# eICU was not included in this re-implementation 





## AI Clinician core code

# (c) Matthieu Komorowski, Imperial College London 2015-2019
# as seen in publication: https://www.nature.com/articles/s41591-018-0213-5

# version 16 Feb 19
# Builds 500 models using MIMIC-III training data
# Records best candidate models along the way from off-policy policy evaluation on MIMIC-III validation data
# Tests the best model on eRI data


# TAKES:
        # MIMICtable = m*59 table with raw values from MIMIC
        # eICUtable = n*56 table with raw values from eICU
        

# GENERATES:
        # MIMICraw = MIMIC RAW DATA m*47 array with columns in right order
        # MIMICzs = MIMIC ZSCORED m*47 array with columns in right order, matching MIMICraw
        # eICUraw = eICU RAW DATA n*47 array with columns in right order, matching MIMICraw
        # eICUzs = eICU ZSCORED n*47 array with columns in right order, matching MIMICraw
        # recqvi = summary statistics of all 500 models
        # idxs = state membership of MIMIC test records, for all 500 models
     	# OA = optimal policy, for all 500 models
        # allpols = detailed data about the best candidate models

# This code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE

# Note: The size of the cohort will depend on which version of MIMIC-III is used.
# The original cohort from the 2018 Nature Medicine publication was built using MIMIC-III v1.3.

import pickle #用于加载/保存python对象
import numpy as np #库可以用于处理多维数组和矩阵运算
import pandas as pd #基于numpy的第三方数据分析库，提供高效数据结构以及数据分析工具
from scipy.stats import zscore, rankdata #scipy库提供优化，积分，差值，统计等多种算法。本语句是从scipy.stats子模块中导入zscore和rankdata两个函数；zscore函数是计算数组中各元素的z分数，表示某个数据点距离均值有多少标准差；rankdata用于计算数组中各元素排名/平均排名/最小排名等。
import math #数学类函数库，仅支持整数和浮点数运算
import scipy.io as sio #语句从scipy库中导入io子模块，并为这个子模块指定一个别名sio；scipy.io可以处理不同格式的数据文件如MATLAB文件，图像文件等；别名方便后续代码书写
import datetime #datetime库可以处理日期和时间
from scipy.stats.mstats import mquantiles #scipy.stats 是 SciPy 库中用于统计分析的子模块，而 mstats 是 stats 里专门处理掩码数组（masked arrays）的子模块（掩码数组是一种特殊的数组，它允许你标记某些元素为无效值，在进行统计计算时可以【忽略这些无效值】）；mquantiles函数用于计算掩码数组的分位数（eg，50百分位数）
from mdptoolbox.mdp import PolicyIteration #此语句的作用是从 mdptoolbox 库的 mdp 子模块里导入PolicyIteration类；mdptoolbox是一个用于解决马尔可夫决策过程（Markov Decision Process, MDP）问题的库；PolicyIteration可以实现策略迭代算法
from reinforcement_learning_mp import offpolicy_multiple_eval_010518 #从文件中导入数据？
from kmeans_mp import kmeans_with_multiple_runs #k-means算法模块
from multiprocessing import freeze_support #freeze_support 函数主要用于支持在 Windows 系统以及其他不支持 fork 系统调用的平台上，将使用 multiprocessing 模块编写的 Python 程序打包成可执行文件（如使用 PyInstaller、cx_Freeze 等工具打包）；在这些平台上，如果不使用 freeze_support，打包后的程序在运行时可能会出现各种问题，比如程序崩溃、无限创建子进程等

def my_zscore(x):  #定义一个my_zscore的函数，x为自变量
    return zscore(x,ddof=1),np.mean(x,axis=0),np.std(x,axis=0,ddof=1) #返回值三个值：数组x经过z分数标准化后的结果（ddof=1表示计算标准差时自由度为1）、数组x沿指定轴的均值（axis=0通常是计算列均值），数组x沿指定轴的标准差


# In[ ]:


######### Functions used in Reinforcement Learning ######## 


class PolicyIteration_with_Q(PolicyIteration): #定义一个子类PolicyIteration_with_Q，继承自父类PolicyIteration；继承意味着 PolicyIteration_with_Q 类会自动拥有 PolicyIteration 类的所有属性和方法，同时还可以添加或修改自己特有的属性和方法
    def __init__(self, transitions, reward, discount, policy0=None,max_iter=1000, eval_type=0, skip_check=False):  #定义类中的属性，transitions表示状态转移概率矩阵；reward为奖励矩阵；discount折扣因子（即γ）；policy0初始策略默认为none；max_iter最大迭代次数默认为1000，为迭代终止条件；eval_type评估类型可能用于指定策略评估阶段的具体方法？默认为0；skip_check是否跳过检查，默认为false，可能用于控制是否对输入的transitions和reward矩阵进行有效性检查
        # Python MDP toolbox from https://github.com/sawcordwell/pymdptoolbox
        # In Matlab MDP Toolbox, P = (S, S, A), R = (S, A) 
        # In Python MDP Toolbox, P = (A, S, S), R= (S, A)

        transitions = np.transpose(transitions,(2,0,1)).copy() # Change to Action First (A, S, S) #np.transpose 是一个函数，用于对数组的维度进行转置操作；它接受两个参数，第一个参数是要进行转置的数组，第二个参数是一个元组，用于指定新的维度顺序；因为原有数组顺序为（S,S,A）,转换为(A,S,S)；（2，0，1）表示新的维度数据，原数组的第二个维度会变成新数组的第0个维度，原数组的第0个维度会变成新数组的第个维度...；对 transitions 数组进行维度转置操作，并且将结果复制一份重新赋值给transitions变量，copy方法是为了创建一个转制后的副本，使用copy可以确保后续对transitions数组的修改不会影响到原数组
        skip_check = True # To Avoid StochasticError: 'PyMDPToolbox - The transition probability matrix is not stochastic.'  在马尔可夫决策过程（MDP）中，转移概率矩阵需要满足随机矩阵的性质，即矩阵的每一行元素之和必须等于 1；PyMDPToolbox 库在进行某些操作时会检查转移概率矩阵是否为随机矩阵，如果不满足条件，就会抛出 StochasticError 异常；将 skip_check 设置为 True 后，可能会跳过对转移概率矩阵的随机矩阵检查，从而避免因矩阵不满足随机矩阵条件而抛出异常；但跳过检查可能会导致后续的计算结果不准确，因为 MDP 算法通常假设转移概率矩阵是随机矩阵

        PolicyIteration.__init__(self, transitions, reward, discount, policy0=None,max_iter=1000, eval_type=0, skip_check=skip_check) #父类自身初始化，将父类初始化放在子类之后可以避免覆盖父类初始化属性的情况发生
    
    def _bellmanOperator_with_Q(self, V=None): #定义一个方法名为 _bellmanOperator_with_Q，且名字前有_表明方法主要用于类内部的实现。V初始值为None，若无V值传入则为默认值，若有则为传入值
        # Apply the Bellman operator on the value function.
        #
        # Updates the value function and the Vprev-improving policy.
        #
        # Returns: (policy, Q, value), tuple of new policy and its value  方法会返回一个元组 (policy, Q, value)，其中 policy 是新的策略，Q 通常代表动作价值函数（Q-function），value 是新策略对应的值函数
        #
        # If V hasn't been sent into the method, then we assume to be working    如果 V 没有被传入方法，那么方法会假设使用类实例对象自身的 V 属性进行计算，即使用类中已经存储的值函数进行贝尔曼算子的应用
        # on the objects V attribute
        if V is None:   #下面这段代码的作用是在 V 为 None 时使用类实例的 V 属性，在 V 不为 None 时检查其是否为 numpy 数组或矩阵，以及形状是否符合要求。
            # this V should be a reference to the data rather than a copy
            V = self.V
        else:
            # make sure the user supplied V is of the right shape
            try:
                assert V.shape in ((self.S,), (1, self.S)), "V is not the "                     "right shape (Bellman operator)."  #检查 V 的形状是否符合要求。self.S 通常代表状态的数量。((self.S,), (1, self.S)) 表示 V 可以是一维数组（形状为 (self.S,)），也可以是二维数组且只有一行（形状为 (1, self.S)）。如果 V 的形状不在这两种情况之内，assert 语句会触发 AssertionError，并输出错误信息 "V is not the right shape (Bellman operator)."
            except AttributeError:  #捕获 AttributeError 异常
                raise TypeError("V must be a numpy array or matrix.")  #当捕获到该异常时，raise TypeError("V must be a numpy array or matrix.") 会抛出一个 TypeError 异常，提示用户 V 必须是 numpy 数组或矩阵
        # Looping through each action the the Q-value matrix is calculated.    代码会遍历每一个动作，进而计算 Q 值矩阵。在马尔可夫决策过程（MDP）中，Q 值表示在某个状态下采取某个动作所能获得的期望累积奖励
        # P and V can be any object that supports indexing, so it is important
        # that you know they define a valid MDP before calling the
        # _bellmanOperator method. Otherwise the results will be meaningless.
        Q = np.empty((self.A, self.S))  #使用 np.empty 函数创建一个形状为 (self.A, self.S) 的空数组 Q，其中 self.A 代表动作的数量，self.S 代表状态的数量。这个数组将用于存储计算得到的 Q 值矩阵
        for aa in range(self.A):  #通过 for 循环遍历每一个动作 aa
            Q[aa] = self.R[aa] + self.discount * self.P[aa].dot(V) #对于每个动作 aa，根据贝尔曼方程计算 Q 值。self.R[aa] 表示在采取动作 aa 时的即时奖励，self.discount 是折扣因子，self.P[aa].dot(V) 表示在采取动作 aa 后，根据状态转移概率矩阵 self.P[aa] 对下一个状态的价值函数 V 进行加权求和。将即时奖励和折扣后的下一个状态的价值相加，得到在该状态下采取该动作的 Q 值
        # Get the policy and value, for now it is being returned but...
        # Which way is better?
        # 1. Return, (policy, value)
        return (Q.argmax(axis=0), Q, Q.max(axis=0)) #直接返回一个元组 (Q.argmax(axis=0), Q, Q.max(axis=0))。Q.argmax(axis=0) 表示在每个状态下选择 Q 值最大的动作，得到新的策略；Q 是计算得到的 Q 值矩阵；Q.max(axis=0) 表示在每个状态下 Q 值的最大值，得到新的价值函数
        # 2. update self.policy and self.V directly
        # self.V = Q.max(axis=1)
        # self.policy = Q.argmax(axis=1)
    
    def run(self):   #到167行这段定义了一个方法run，用来实现策略迭代算法
        # Run the policy iteration algorithm.
        self._startRun()  #可能在父类中？/某个已经定义好的类中的方法函数

        while True:   #创建无限循环
            self.iter += 1  #每迭代一次self.iter加1
            # these _evalPolicy* functions will update the classes value
            # attribute
            if self.eval_type == "matrix":  #根据self.eval_type的值选择不同的策略评估方法，若为matrix则调用self._evalPolicyMatrix方法（可能是矩阵计算？）
                self._evalPolicyMatrix()
            elif self.eval_type == "iterative":  #若为iterative则调用self._evalPolicyIterative方法（可能是迭代的方式计算？）
                self._evalPolicyIterative()
            # This should update the classes policy attribute but leave the
            # value alone
            policy_next, Q, null = self._bellmanOperator_with_Q()  #调用self._bellmanOperator_with_Q方法，该方法会返回新的策略policy_next, Q矩阵和null值
            del null  #删除null变量，释放内存
            # calculate in how many places does the old policy disagree with  #计算新旧策略（policy_next与self.policy）在多少个状态上不一样，赋值给n_different
            # the new policy
            n_different = (policy_next != self.policy).sum()  #计算策略间差异
            # if verbose then continue printing a table
            if self.verbose:  #verbose是一个控制输出的标志变量，设为true可以输出详细的运行信息排查问题，设为false则可以避免不必要的输出
                _printVerbosity(self.iter, n_different)  #如果self.verbose为True，则调用 _printVerbosity 方法，输出当前迭代次数和策略差异的详细信息
            # Once the policy is unchanging of the maximum number of
            # of iterations has been reached then stop
            if n_different == 0:   #以下为终止条件判断 如果n_different=0则说明新旧策略完全相同策略已经收敛，此时跳出循环
                if self.verbose:  #输出对应的文字policy unchanging. Stopping iteration.
                    print(_MSG_STOP_UNCHANGING_POLICY)
                break
            elif self.iter == self.max_iter:  或者self.iter等于self.max_iter说明达到预设的最大迭代次数，跳出循环并打印Maximum number of iterations reached. Stopping optimization
                if self.verbose:
                    print(_MSG_STOP_MAX_ITER)
                break
            elif self.iter > 20 and n_different <=5 : # This condition was added from the Nature Code   #如果迭代次数大于20且策略差异小于5，也停止迭代并打印Training stopped early
                if self.verbose: 
                    print((_MSG_STOP))  #多一层括号？
                break 
            else:  #如果不符合上面的终止条件将policy_next赋值给self.policy继续迭代
                self.policy = policy_next 

        self._endRun()
        
        return Q 
    


# In[ ]:


if __name__ == '__main__': 
    
    freeze_support()
    
    # To ignore 'Runtime Warning: Invalid value encountered in greater' caused by NaN 
    np.warnings.filterwarnings('ignore')

    # Load pickle 
    with open('step_4_start.pkl', 'rb') as file:
        MIMICtable = pickle.load(file)
    
    #############################  MODEL PARAMETERS   #####################################

    print('####  INITIALISATION  ####') 

    nr_reps=500               # nr of repetitions (total nr models) % 500 
    nclustering=32            # how many times we do clustering (best solution will be chosen) % 32
    prop=0.25                 # proportion of the data we sample for clustering
    gamma=0.99                # gamma
    transthres=5              # threshold for pruning the transition matrix
    polkeep=1                 # count of saved policies
    ncl=750                   # nr of states
    nra=5                     # nr of actions (2 to 10)
    ncv=5                     # nr of crossvalidation runs (each is 80% training / 20% test)
    OA=np.full((752,nr_reps),np.nan)       # record of optimal actions
    recqvi=np.full((nr_reps*2,30),np.nan)  # saves data about each model (1 row per model)
    # allpols=[]  # saving best candidate models
    
    
    # #################   Convert training data and compute conversion factors    ######################

    # all 47 columns of interest
    colbin = ['gender','mechvent','max_dose_vaso','re_admission'] 
    colnorm= ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance'] 
    collog=['SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total','input_4hourly','output_total','output_4hourly']

    colbin=np.where(np.isin(MIMICtable.columns,colbin))[0]
    colnorm=np.where(np.isin(MIMICtable.columns,colnorm))[0]
    collog=np.where(np.isin(MIMICtable.columns,collog))[0]

    # find patients who died in ICU during data collection period
    # ii=MIMICtable.bloc==1&MIMICtable.died_within_48h_of_out_time==1& MIMICtable.delay_end_of_record_and_discharge_or_death<24;
    # icustayidlist=MIMICtable.icustayid;
    # ikeep=~ismember(icustayidlist,MIMICtable.icustayid(ii));
    reformat5=MIMICtable.values.copy() 
    # reformat5=reformat5(ikeep,:);
    icustayidlist=MIMICtable['icustayid']
    icuuniqueids=np.unique(icustayidlist) # list of unique icustayids from MIMIC
    idxs=np.full((icustayidlist.shape[0],nr_reps),np.nan) # record state membership test cohort

    MIMICraw=MIMICtable.iloc[:, np.concatenate([colbin,colnorm,collog])] 
    MIMICraw=MIMICraw.values.copy()  # RAW values
    MIMICzs=np.concatenate([reformat5[:, colbin]-0.5, zscore(reformat5[:,colnorm],ddof=1), zscore(np.log(0.1+reformat5[:, collog]),ddof=1)],axis=1)
    MIMICzs[:,3]=np.log(MIMICzs[:,3]+0.6)   # MAX DOSE NORAD 
    MIMICzs[:,44]=2*MIMICzs[:,44]   # increase weight of this variable


    # eICU section was not implemented 
    
    # compute conversion factors using MIMIC data
    a=MIMICraw[:, 0:3]-0.5 
    b= np.log(MIMICraw[:,3]+0.1)
    c,cmu,csigma = my_zscore(MIMICraw[:,4:36])
    d,dmu,dsigma = my_zscore(np.log(0.1+MIMICraw[:,36:47]))
    

    ####################### Main LOOP ###########################
    bestpol = 0 
    
    for modl in range(nr_reps):  # MAIN LOOP OVER ALL MODELS
        N=icuuniqueids.size # total number of rows to choose from
        grp=np.floor(ncv*np.random.rand(N,1)+1);  #list of 1 to 5 (20% of the data in each grp) -- this means that train/test MIMIC split are DIFFERENT in all the 500 models
        crossval=1;
        trainidx=icuuniqueids[np.where(grp!=crossval)[0]]
        testidx=icuuniqueids[np.where(grp==crossval)[0]]
        train=np.isin(icustayidlist,trainidx)
        test=np.isin(icustayidlist,testidx)
        X=MIMICzs[train,:]
        Xtestmimic=MIMICzs[~train,:]
        blocs=reformat5[train,0]
        bloctestmimic=reformat5[~train,0]
        ptid=reformat5[train,1]
        ptidtestmimic=reformat5[~train,1] 
        outcome=9 #   HOSP _ MORTALITY = 7 / 90d MORTA = 9
        Y90=reformat5[train,outcome];   


        print('########################   MODEL NUMBER : ',modl)
        print(datetime.datetime.now())

        #######   find best clustering solution (lowest intracluster variability)  ####################
        print('####  CLUSTERING  ####') # BY SAMPLING
        N=X.shape[0] #total number of rows to choose from
        sampl=X[np.where(np.floor(np.random.rand(N,1)+prop))[0],:]

        C = kmeans_with_multiple_runs(ncl,10000,nclustering,sampl) 
        idx = C.predict(X)

        ############################## CREATE ACTIONS  ########################
        print('####  CREATE ACTIONS  ####') 

        nact=nra*nra

        iol=MIMICtable.columns.get_loc('input_4hourly') 
        vcl=MIMICtable.columns.get_loc('max_dose_vaso') 

        a= reformat5[:,iol].copy()                   # IV fluid
        a= rankdata(a[a>0])/a[a>0].shape[0]   # excludes zero fluid (will be action 1)

        iof=np.floor((a+0.2499999999)*4)  #converts iv volume in 4 actions

        a= reformat5[:,iol].copy() 
        a= np.where(a>0)[0]  # location of non-zero fluid in big matrix

        io=np.ones((reformat5.shape[0],1))  # array of ones, by default     
        io[a]=(iof+1).reshape(-1,1)   # where more than zero fluid given: save actual action
        io = io.ravel() 

        vc=reformat5[:,vcl].copy() 
        vcr= rankdata(vc[vc!=0])/vc[vc!=0].size
        vcr=np.floor((vcr+0.249999999999)*4)  # converts to 4 bins
        vcr[vcr==0]=1
        vc[vc!=0]=vcr+1 
        vc[vc==0]=1

        ma1 = np.array([np.median(reformat5[io==1,iol]),np.median(reformat5[io==2,iol]),np.median(reformat5[io==3,iol]), np.median(reformat5[io==4,iol]),np.median(reformat5[io==5,iol])]) # median dose of drug in all bins
        ma2 = np.array([np.median(reformat5[vc==1,vcl]),np.median(reformat5[vc==2,vcl]),np.median(reformat5[vc==3,vcl]), np.median(reformat5[vc==4,vcl]),np.median(reformat5[vc==5,vcl])])

        med = np.concatenate([io.reshape(-1,1),vc.reshape(-1,1)],axis=1)
        uniqueValues,actionbloc = np.unique(med,axis=0,return_inverse=True)

        actionbloctrain=actionbloc[train] 

        ma2Values = ma2[uniqueValues[:,1].astype('int64')-1].reshape(-1,1)
        ma1Values = ma1[uniqueValues[:,0].astype('int64')-1].reshape(-1,1)

        uniqueValuesdose = np.concatenate([ma2Values,ma1Values],axis=1) # median dose of each bin for all 25 actions 

        ####################################################################################################################################
        print('####  CREATE QLDATA3  ####')

        r=np.array([100, -100]).reshape(1,-1)
        r2=r*(2*(1-Y90.reshape(-1,1))-1)
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)
        qldata=np.concatenate([blocs.reshape(-1,1), idx.reshape(-1,1), actionbloctrain.reshape(-1,1), Y90.reshape(-1,1), r2],axis=1)  # contains bloc / state / action / outcome&reward     
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),4))
        c=-1
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 

        for i in range(qldata.shape[0]-1):
            c=c+1
            qldata3[c,:]=qldata[i,0:4] 
            if(qldata[i+1,0]==1): #end of trace for this patient
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,4]]) 

        qldata3=qldata3[:c+1,:]


        # ###################################################################################################################################
        print("####  CREATE TRANSITION MATRIX T(S'',S,A) ####")
        transitionr=np.zeros((ncl+2,ncl+2,nact))  #this is T(S',S,A)
        sums0a0=np.zeros((ncl+2,nact)) 

        for i in range(qldata3.shape[0]-1):    
            if (qldata3[i+1,0]!=1) : # if we are not in the last state for this patient = if there is a transition to make!
                S0=int(qldata3[i,1]) 
                S1=int(qldata3[i+1,1])
                acid= int(qldata3[i,2]) 
                transitionr[S1,S0,acid]=transitionr[S1,S0,acid]+1 
                sums0a0[S0,acid]=sums0a0[S0,acid]+1

        sums0a0[sums0a0<=transthres]=0  #delete rare transitions (those seen less than 5 times = bottom 50%!!)

        for i in range(ncl+2): 
            for j in range(nact): 
                if sums0a0[i,j]==0: 
                    transitionr[:,i,j]=0; 
                else:
                    transitionr[:,i,j]=transitionr[:,i,j]/sums0a0[i,j]


        transitionr[np.isnan(transitionr)]=0  #replace NANs with zeros
        transitionr[np.isinf(transitionr)]=0  #replace NANs with zeros

        physpol=sums0a0/np.sum(sums0a0, axis=1).reshape(-1,1)    #physicians policy: what action was chosen in each state


        print("####  CREATE TRANSITION MATRIX T(S,S'',A)  ####")

        transitionr2=np.zeros((ncl+2,ncl+2,nact))  # this is T(S,S',A)
        sums0a0=np.zeros((ncl+2,nact))

        for i in range(qldata3.shape[0]-1) : 
            if (qldata3[i+1,0]!=1) : # if we are not in the last state for this patient = if there is a transition to make!
                S0=int(qldata3[i,1])
                S1=int(qldata3[i+1,1])
                acid= int(qldata3[i,2]) 
                transitionr2[S0,S1,acid]=transitionr2[S0,S1,acid]+1;  
                sums0a0[S0,acid]=sums0a0[S0,acid]+1

        sums0a0[sums0a0<=transthres]=0;  #delete rare transitions (those seen less than 5 times = bottom 50%!!) IQR = 2-17

        for i in range(ncl+2): 
            for j in range(nact): 
                if sums0a0[i,j]==0:
                    transitionr2[i,:,j]=0 
                else: 
                    transitionr2[i,:,j]=transitionr2[i,:,j]/sums0a0[i,j]

        transitionr2[np.isnan(transitionr2)]=0 #replace NANs with zeros
        transitionr2[np.isinf(transitionr2)]=0 # replace infs with zeros

        print('####  CREATE REWARD MATRIX  R(S,A) ####')
        # CF sutton& barto bottom 1998 page 106. i compute R(S,A) from R(S'SA) and T(S'SA)
        r3=np.zeros((ncl+2,ncl+2,nact))
        r3[ncl,:,:]=-100
        r3[ncl+1,:,:]=100
        R=sum(transitionr*r3)
        R=np.squeeze(R)   #remove 1 unused dimension


        print('####  POLICY ITERATION   ####')


        pi = PolicyIteration_with_Q(transitionr2, R, gamma, np.ones((ncl+2,1))) 
        Qon = np.transpose(pi.run()) 
        OptimalAction=np.argmax(Qon,axis=1).reshape(-1,1)  #deterministic 
        OA[:,modl]=OptimalAction.ravel() #save optimal actions


        print('####  OFF-POLICY EVALUATION - MIMIC TRAIN SET ####')
        # create new version of QLDATA3

        r=np.array([100, -100]).reshape(1,-1)
        r2=r*(2*(1-Y90.reshape(-1,1))-1)
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)
        qldata=np.concatenate([blocs.reshape(-1,1), idx.reshape(-1,1), actionbloctrain.reshape(-1,1), Y90.reshape(-1,1),np.zeros((idx.size,1)), r2[:,0].reshape(-1,1), ptid.reshape(-1,1) ],axis=1)   # contains bloc / state / action / outcome&reward     
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),8))
        c=-1
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 

        for i in range(qldata.shape[0]-1):
            c=c+1
            qldata3[c,:]=qldata[i,[0,1,2,4,6,6,6,6]] 
            if(qldata[i+1,0]==1): #end of trace for this patient
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,5],0,0,-1,qldata[i,6]]) 

        qldata3=qldata3[:c+1,:]

        #  add pi(s,a) and b(s,a)
        p=0.01 #softening policies 
        softpi=physpol.copy() # behavior policy = clinicians' 
        for i in range(ncl): 
            ii=softpi[i,:]==0    
            z=p/sum(ii)    
            nz=p/sum(~ii)    
            softpi[i,ii]=z;   
            softpi[i,~ii]=softpi[i,~ii]-nz;

        softb=np.abs(np.zeros((ncl+2,nact))-p/24) #"optimal" policy = target policy = evaluation policy 

        for i in range(ncl): 
            softb[i,OptimalAction[i]]=1-p


        for i in range(qldata3.shape[0]):  # adding the probas of policies to qldata3
            if qldata3[i,1]<ncl :
                qldata3[i,4]=softpi[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,5]=softb[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,6]=OptimalAction[int(qldata3[i,1])]  #optimal action

        qldata3train=qldata3.copy() 


        bootql,bootwis = offpolicy_multiple_eval_010518(qldata3,physpol, 0.99,1,6,750)

        recqvi[modl,0]=modl
        recqvi[modl,3]=np.nanmean(bootql)
        recqvi[modl,4]=mquantiles(bootql,0.99, alphap=0.5, betap=0.5)[0]
        recqvi[modl,5]=np.nanmean(bootwis) # we want this as high as possible
        recqvi[modl,6]=mquantiles(bootwis,0.05, alphap=0.5, betap=0.5)[0] #we want this as high as possible


        # testing on MIMIC-test
        print('####  OFF-POLICY EVALUATION - MIMIC TEST SET ####')

        # create new version of QLDATA3 with MIMIC TEST samples

        idxtest = C.predict(Xtestmimic)

        idxs[test,modl]=idxtest.ravel()  #important: record state membership of test cohort

        actionbloctest=actionbloc[~train] 
        Y90test=reformat5[~train,outcome]

        r=np.array([100, -100]).reshape(1,-1)
        r2=r*(2*(1-Y90test.reshape(-1,1))-1)
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)

        qldata=np.concatenate([bloctestmimic.reshape(-1,1), idxtest.reshape(-1,1), actionbloctest.reshape(-1,1), Y90test.reshape(-1,1),np.zeros((idxtest.size,1)), r2[:,0].reshape(-1,1), ptidtestmimic.reshape(-1,1) ],axis=1)   # contains bloc / state / action / outcome&reward     
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),8))
        c=-1
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 

        for i in range(qldata.shape[0]-1):
            c=c+1
            qldata3[c,:]=qldata[i,[0,1,2,4,6,6,6,6]] 
            if(qldata[i+1,0]==1): #end of trace for this patient
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,5],0,0,-1,qldata[i,6]]) 

        qldata3=qldata3[:c+1,:]

        #  add pi(s,a) and b(s,a)
        p=0.01 # small correction factor #softening policies 
        softpi=physpol.copy() # behavior policy = clinicians' 
        for i in range(ncl): 
            ii=softpi[i,:]==0    
            z=p/sum(ii)    
            nz=p/sum(~ii)    
            softpi[i,ii]=z;   
            softpi[i,~ii]=softpi[i,~ii]-nz;

        softb=np.abs(np.zeros((ncl+2,nact))-p/24) #"optimal" policy = target policy = evaluation policy 

        for i in range(ncl): 
            softb[i,OptimalAction[i]]=1-p

        for i in range(qldata3.shape[0]):  # adding the probas of policies to qldata3
            if qldata3[i,1]<ncl :
                qldata3[i,4]=softpi[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,5]=softb[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,6]=OptimalAction[int(qldata3[i,1])]  #optimal action

        qldata3test=qldata3.copy() 

        bootmimictestql,bootmimictestwis = offpolicy_multiple_eval_010518(qldata3,physpol, 0.99,1,6,2000)

        recqvi[modl,18]=mquantiles(bootmimictestql,0.95, alphap=0.5, betap=0.5)[0] #PHYSICIANS' 95% UB
        recqvi[modl,19]=np.nanmean(bootmimictestql)
        recqvi[modl,20]=mquantiles(bootmimictestql,0.99, alphap=0.5, betap=0.5)[0]
        recqvi[modl,21]=np.nanmean(bootmimictestwis) 
        recqvi[modl,22]=mquantiles(bootmimictestwis,0.01, alphap=0.5, betap=0.5)[0] 
        recqvi[modl,23]=mquantiles(bootmimictestwis,0.05, alphap=0.5, betap=0.5)[0] #AI 95% LB, we want this as high as possible




        if recqvi[modl,23] > 40: #saves time if policy is not good on MIMIC test: skips to next model
            print('########################## eICU TEST SET #############################')
            # eICU part was not implemented 



        # eICU testing was not included 
        if recqvi[modl,23]>0 : #  & recqvi(modl,14)>0   # if 95% LB is >0 : save the model (otherwise it's pointless)
            print('####   GOOD MODEL FOUND - SAVING IT   ####' ) 

            # best pol 
            if(bestpol < recqvi[modl,23]):
                print('Best policy was replaced => 95% LB is ',recqvi[modl,23])
                bestpol = recqvi[modl,23] 
                
                # save to pickle 
                with open('bestpol.pkl', 'wb') as file:
                    pickle.dump(modl,file)
                    pickle.dump(Qon,file)
                    pickle.dump(physpol,file)
                    pickle.dump(transitionr,file) 
                    pickle.dump(transitionr2,file)
                    pickle.dump(R,file)
                    pickle.dump(C,file)
                    pickle.dump(train,file)
                    pickle.dump(qldata3train,file)
                    pickle.dump(qldata3test,file) 


    recqvi=recqvi[:modl+1,:]

    # save to pickle for visualization 
    with open('step_5_start.pkl', 'wb') as file:
        pickle.dump(MIMICzs,file)
        pickle.dump(actionbloc,file)
        pickle.dump(reformat5,file)
        pickle.dump(recqvi,file)

    # save recqvi in csv format 
    np.savetxt('recqvi.csv',recqvi,delimiter=',') 



# In[ ]:




