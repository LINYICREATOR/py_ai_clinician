#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Re-implmentation of AI Clinician Matlab Code in Python 
# Author: KyungJoong Kim (GIST, South Korea)
# Date: 2020 June 2 
# 
# This code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE


# Note 
#
# K-Means in scikit-learn will produce differnt outcome with Matlab's original K-means 
# The random number generator will produce different random number sequence 
# 
# Todo: 
# eICU was not included in this re-implementation 





## AI Clinician core code

# (c) Matthieu Komorowski, Imperial College London 2015-2019
# as seen in publication: https://www.nature.com/articles/s41591-018-0213-5

# version 16 Feb 19
# Builds 500 models using MIMIC-III training data
# Records best candidate models along the way from off-policy policy evaluation on MIMIC-III validation data
# Tests the best model on eRI data


# TAKES:
        # MIMICtable = m*59 table with raw values from MIMIC
        # eICUtable = n*56 table with raw values from eICU
        

# GENERATES:
        # MIMICraw = MIMIC RAW DATA m*47 array with columns in right order
        # MIMICzs = MIMIC ZSCORED m*47 array with columns in right order, matching MIMICraw
        # eICUraw = eICU RAW DATA n*47 array with columns in right order, matching MIMICraw
        # eICUzs = eICU ZSCORED n*47 array with columns in right order, matching MIMICraw
        # recqvi = summary statistics of all 500 models
        # idxs = state membership of MIMIC test records, for all 500 models
     	# OA = optimal policy, for all 500 models
        # allpols = detailed data about the best candidate models

# This code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE

# Note: The size of the cohort will depend on which version of MIMIC-III is used.
# The original cohort from the 2018 Nature Medicine publication was built using MIMIC-III v1.3.

import pickle #用于加载/保存python对象
import numpy as np #库可以用于处理多维数组和矩阵运算
import pandas as pd #基于numpy的第三方数据分析库，提供高效数据结构以及数据分析工具
from scipy.stats import zscore, rankdata #scipy库提供优化，积分，差值，统计等多种算法。本语句是从scipy.stats子模块中导入zscore和rankdata两个函数；zscore函数是计算数组中各元素的z分数，表示某个数据点距离均值有多少标准差；rankdata用于计算数组中各元素排名/平均排名/最小排名等。
import math #数学类函数库，仅支持整数和浮点数运算
import scipy.io as sio #语句从scipy库中导入io子模块，并为这个子模块指定一个别名sio；scipy.io可以处理不同格式的数据文件如MATLAB文件，图像文件等；别名方便后续代码书写
import datetime #datetime库可以处理日期和时间
from scipy.stats.mstats import mquantiles #scipy.stats 是 SciPy 库中用于统计分析的子模块，而 mstats 是 stats 里专门处理掩码数组（masked arrays）的子模块（掩码数组是一种特殊的数组，它允许你标记某些元素为无效值，在进行统计计算时可以【忽略这些无效值】）；mquantiles函数用于计算掩码数组的分位数（eg，50百分位数）
from mdptoolbox.mdp import PolicyIteration #此语句的作用是从 mdptoolbox 库的 mdp 子模块里导入PolicyIteration类；mdptoolbox是一个用于解决马尔可夫决策过程（Markov Decision Process, MDP）问题的库；PolicyIteration可以实现策略迭代算法
from reinforcement_learning_mp import offpolicy_multiple_eval_010518 #从文件中导入数据？
from kmeans_mp import kmeans_with_multiple_runs #k-means算法模块
from multiprocessing import freeze_support #freeze_support 函数主要用于支持在 Windows 系统以及其他不支持 fork 系统调用的平台上，将使用 multiprocessing 模块编写的 Python 程序打包成可执行文件（如使用 PyInstaller、cx_Freeze 等工具打包）；在这些平台上，如果不使用 freeze_support，打包后的程序在运行时可能会出现各种问题，比如程序崩溃、无限创建子进程等

def my_zscore(x):  #定义一个my_zscore的函数，x为自变量
    return zscore(x,ddof=1),np.mean(x,axis=0),np.std(x,axis=0,ddof=1) #返回值三个值：数组x经过z分数标准化后的结果（ddof=1表示计算标准差时自由度为1）、数组x沿指定轴的均值（axis=0通常是计算列均值），数组x沿指定轴的标准差


# In[ ]:


######### Functions used in Reinforcement Learning ########   强化学习中用到的函数


class PolicyIteration_with_Q(PolicyIteration): #定义一个子类PolicyIteration_with_Q，继承自父类PolicyIteration；继承意味着 PolicyIteration_with_Q 类会自动拥有 PolicyIteration 类的所有属性和方法，同时还可以添加或修改自己特有的属性和方法
    def __init__(self, transitions, reward, discount, policy0=None,max_iter=1000, eval_type=0, skip_check=False):  #定义类中的属性，transitions表示状态转移概率矩阵；reward为奖励矩阵；discount折扣因子（即γ）；policy0初始策略默认为none；max_iter最大迭代次数默认为1000，为迭代终止条件；eval_type评估类型可能用于指定策略评估阶段的具体方法？默认为0；skip_check是否跳过检查，默认为false，可能用于控制是否对输入的transitions和reward矩阵进行有效性检查
        # Python MDP toolbox from https://github.com/sawcordwell/pymdptoolbox
        # In Matlab MDP Toolbox, P = (S, S, A), R = (S, A) 
        # In Python MDP Toolbox, P = (A, S, S), R= (S, A)

        transitions = np.transpose(transitions,(2,0,1)).copy() # Change to Action First (A, S, S) #np.transpose 是一个函数，用于对数组的维度进行转置操作；它接受两个参数，第一个参数是要进行转置的数组，第二个参数是一个元组，用于指定新的维度顺序；因为原有数组顺序为（S,S,A）,转换为(A,S,S)；（2，0，1）表示新的维度数据，原数组的第二个维度会变成新数组的第0个维度，原数组的第0个维度会变成新数组的第个维度...；对 transitions 数组进行维度转置操作，并且将结果复制一份重新赋值给transitions变量，copy方法是为了创建一个转制后的副本，使用copy可以确保后续对transitions数组的修改不会影响到原数组
        skip_check = True # To Avoid StochasticError: 'PyMDPToolbox - The transition probability matrix is not stochastic.'  在马尔可夫决策过程（MDP）中，转移概率矩阵需要满足随机矩阵的性质，即矩阵的每一行元素之和必须等于 1；PyMDPToolbox 库在进行某些操作时会检查转移概率矩阵是否为随机矩阵，如果不满足条件，就会抛出 StochasticError 异常；将 skip_check 设置为 True 后，可能会跳过对转移概率矩阵的随机矩阵检查，从而避免因矩阵不满足随机矩阵条件而抛出异常；但跳过检查可能会导致后续的计算结果不准确，因为 MDP 算法通常假设转移概率矩阵是随机矩阵

        PolicyIteration.__init__(self, transitions, reward, discount, policy0=None,max_iter=1000, eval_type=0, skip_check=skip_check) #父类自身初始化，将父类初始化放在子类之后可以避免覆盖父类初始化属性的情况发生
    
    def _bellmanOperator_with_Q(self, V=None): #定义一个方法名为 _bellmanOperator_with_Q，且名字前有_表明方法主要用于类内部的实现。V初始值为None，若无V值传入则为默认值，若有则为传入值
        # Apply the Bellman operator on the value function.
        #
        # Updates the value function and the Vprev-improving policy.
        #
        # Returns: (policy, Q, value), tuple of new policy and its value  方法会返回一个元组 (policy, Q, value)，其中 policy 是新的策略，Q 通常代表动作价值函数（Q-function），value 是新策略对应的值函数
        #
        # If V hasn't been sent into the method, then we assume to be working    如果 V 没有被传入方法，那么方法会假设使用类实例对象自身的 V 属性进行计算，即使用类中已经存储的值函数进行贝尔曼算子的应用
        # on the objects V attribute
        if V is None:   #下面这段代码的作用是在 V 为 None 时使用类实例的 V 属性，在 V 不为 None 时检查其是否为 numpy 数组或矩阵，以及形状是否符合要求。
            # this V should be a reference to the data rather than a copy
            V = self.V
        else:
            # make sure the user supplied V is of the right shape
            try:
                assert V.shape in ((self.S,), (1, self.S)), "V is not the "                     "right shape (Bellman operator)."  #检查 V 的形状是否符合要求。self.S 通常代表状态的数量。((self.S,), (1, self.S)) 表示 V 可以是一维数组（形状为 (self.S,)），也可以是二维数组且只有一行（形状为 (1, self.S)）。如果 V 的形状不在这两种情况之内，assert 语句会触发 AssertionError，并输出错误信息 "V is not the right shape (Bellman operator)."
            except AttributeError:  #捕获 AttributeError 异常
                raise TypeError("V must be a numpy array or matrix.")  #当捕获到该异常时，raise TypeError("V must be a numpy array or matrix.") 会抛出一个 TypeError 异常，提示用户 V 必须是 numpy 数组或矩阵
        # Looping through each action the the Q-value matrix is calculated.    代码会遍历每一个动作，进而计算 Q 值矩阵。在马尔可夫决策过程（MDP）中，Q 值表示在某个状态下采取某个动作所能获得的期望累积奖励
        # P and V can be any object that supports indexing, so it is important
        # that you know they define a valid MDP before calling the
        # _bellmanOperator method. Otherwise the results will be meaningless.
        Q = np.empty((self.A, self.S))  #使用 np.empty 函数创建一个形状为 (self.A, self.S) 的空数组 Q，其中 self.A 代表动作的数量，self.S 代表状态的数量。这个数组将用于存储计算得到的 Q 值矩阵
        for aa in range(self.A):  #通过 for 循环遍历每一个动作 aa
            Q[aa] = self.R[aa] + self.discount * self.P[aa].dot(V) #对于每个动作 aa，根据贝尔曼方程计算 Q 值。self.R[aa] 表示在采取动作 aa 时的即时奖励，self.discount 是折扣因子，self.P[aa].dot(V) 表示在采取动作 aa 后，根据状态转移概率矩阵 self.P[aa] 对下一个状态的价值函数 V 进行加权求和。将即时奖励和折扣后的下一个状态的价值相加，得到在该状态下采取该动作的 Q 值
        # Get the policy and value, for now it is being returned but...
        # Which way is better?
        # 1. Return, (policy, value)
        return (Q.argmax(axis=0), Q, Q.max(axis=0)) #直接返回一个元组 (Q.argmax(axis=0), Q, Q.max(axis=0))。Q.argmax(axis=0) 表示在每个状态下选择 Q 值最大的动作，得到新的策略；Q 是计算得到的 Q 值矩阵；Q.max(axis=0) 表示在每个状态下 Q 值的最大值，得到新的价值函数
        # 2. update self.policy and self.V directly
        # self.V = Q.max(axis=1)
        # self.policy = Q.argmax(axis=1)
    
    def run(self):   #到167行这段定义了一个方法run，用来实现策略迭代算法
        # Run the policy iteration algorithm.
        self._startRun()  #可能在父类中？/某个已经定义好的类中的方法函数

        while True:   #创建无限循环
            self.iter += 1  #每迭代一次self.iter加1
            # these _evalPolicy* functions will update the classes value
            # attribute
            if self.eval_type == "matrix":  #根据self.eval_type的值选择不同的策略评估方法，若为matrix则调用self._evalPolicyMatrix方法（可能是矩阵计算？）
                self._evalPolicyMatrix()
            elif self.eval_type == "iterative":  #若为iterative则调用self._evalPolicyIterative方法（可能是迭代的方式计算？）
                self._evalPolicyIterative()
            # This should update the classes policy attribute but leave the
            # value alone
            policy_next, Q, null = self._bellmanOperator_with_Q()  #调用self._bellmanOperator_with_Q方法，该方法会返回新的策略policy_next, Q矩阵和null值
            del null  #删除null变量，释放内存
            # calculate in how many places does the old policy disagree with  #计算新旧策略（policy_next与self.policy）在多少个状态上不一样，赋值给n_different
            # the new policy
            n_different = (policy_next != self.policy).sum()  #计算策略间差异
            # if verbose then continue printing a table
            if self.verbose:  #verbose是一个控制输出的标志变量，设为true可以输出详细的运行信息排查问题，设为false则可以避免不必要的输出
                _printVerbosity(self.iter, n_different)  #如果self.verbose为True，则调用 _printVerbosity 方法，输出当前迭代次数和策略差异的详细信息
            # Once the policy is unchanging of the maximum number of
            # of iterations has been reached then stop
            if n_different == 0:   #以下为终止条件判断 如果n_different=0则说明新旧策略完全相同策略已经收敛，此时跳出循环
                if self.verbose:  #输出对应的文字policy unchanging. Stopping iteration.
                    print(_MSG_STOP_UNCHANGING_POLICY)
                break
            elif self.iter == self.max_iter:  或者self.iter等于self.max_iter说明达到预设的最大迭代次数，跳出循环并打印Maximum number of iterations reached. Stopping optimization
                if self.verbose:
                    print(_MSG_STOP_MAX_ITER)
                break
            elif self.iter > 20 and n_different <=5 : # This condition was added from the Nature Code   #如果迭代次数大于20且策略差异小于5，也停止迭代并打印Training stopped early
                if self.verbose: 
                    print((_MSG_STOP))  #多一层括号？
                break 
            else:  #如果不符合上面的终止条件将policy_next赋值给self.policy继续迭代
                self.policy = policy_next 

        self._endRun()  调用_endrun方法（可能用于清理资源？）
        
        return Q  #返回最终得到的Q值
    


# In[ ]:
#174-183行主要实现了在主程序中加载step_4_start.pkl文件中数据，同时处理打包和警告过滤相关的问题

if __name__ == '__main__':   #确保代码块里的代码只有在该 Python 文件被直接运行时才会执行，若作为模块被导入则不会执行，这样可以（1）防止不必要的执行，通常导入模块时只希望导入模块的功能（2）大型项目中通常有一个主程序文件作为程序的入口点，可以明确哪些代码是程序起始执行部分？
    
    freeze_support()  #freeze_support函数主要用于支持将 Python 脚本打包成可执行文件，没有导入multiprocessing模块？最前面有
    
    # To ignore 'Runtime Warning: Invalid value encountered in greater' caused by NaN 
    np.warnings.filterwarnings('ignore')  #为了忽略 'Runtime Warning: Invalid value encountered in greater' 警告

    # Load pickle 
    with open('step_4_start.pkl', 'rb') as file:  #使用 with 语句打开一个二进制文件 step_4_start.pkl，'rb' 表示以二进制只读模式打开文件。with 语句会自动管理文件的打开和关闭，保证文件操作结束后会正确关闭文件
        MIMICtable = pickle.load(file)   #使用 pickle 模块的 load 函数从打开的文件中加载数据，并将其赋值给变量 MIMICtable
    
    #############################  MODEL PARAMETERS   #####################################   模型参数设置

    print('####  INITIALISATION  ####')    #提示进行初始化操作

    nr_reps=500               # nr of repetitions (total nr models) % 500   总共要训练的模型数量，500
    nclustering=32            # how many times we do clustering (best solution will be chosen) % 32   表示聚类操作的次数，程序会从聚类结果中选择最优的解决方案，设为32
    prop=0.25                 # proportion of the data we sample for clustering    用于聚类的数据采样比例，设置为0.25
    gamma=0.99                # gamma  折扣因子
    transthres=5              # threshold for pruning the transition matrix   用于修剪转移矩阵的阈值，当转移矩阵中的某些元素满足特定条件（可能是小于该阈值）时，会被修剪掉
    polkeep=1                 # count of saved policies   保存的策略数量，这里设置为 1，表示只保存一个最优策略
    ncl=750                   # nr of states   状态的数量，设置为 750
    nra=5                     # nr of actions (2 to 10)   ？这是哪个参数
    ncv=5                     # nr of crossvalidation runs (each is 80% training / 20% test)   #交叉验证的运行次数，每次交叉验证会将数据划分为 80% 的训练集和 20% 的测试集，这里设置为 5
    OA=np.full((752,nr_reps),np.nan)       # record of optimal actions  使用 numpy 的 full 函数创建一个形状为 (752, nr_reps) 的二维数组，初始值都为 NaN（非数字），用于记录最优动作
    recqvi=np.full((nr_reps*2,30),np.nan)  # saves data about each model (1 row per model)  使用 numpy 的 full 函数创建一个形状为 (nr_reps*2, 30) 的二维数组，初始值为 NaN，用于保存每个模型的相关数据，每行代表一个模型
    # allpols=[]  # saving best candidate models

#插入关于k_means聚类的介绍，文中是通过聚类算法将患者数据聚类为750个离散状态，并通过主成分分析来验证状态分布与死亡率之间的关系；聚类本身数据没有标签
  #k_means聚类是通过迭代的方式：先设置k个质心，数据点依据当前质心被分到某聚类中，再根据当前数据点重新计算聚类的质心
  #输入结果簇的个数k和包含n个对象的集合，输出是k个簇的集合
#插入关于数据集的划分问题：一部分数据用来训练模型让模型能够达到该有的功能以及预测效果，然后用测试集来验证模型的预测效果到底如何。通常两个数据集的比例为7：3或者8：2
  #但通常解决一个问题的算法有很多，模型也有很多，调整模型中的超参（参数的参数）也可以得到不同的模型参数，但哪一种模型最好？引入验证集，将训练集数据分为两部分，训练集和验证集，训练集用于训练模型，而验证集用于给不同模型打分，然后将两部分数据合在一起重新训练最后使用测试集来验证；通常使用的训练集验证集测试集的比例为70：15：15/80：10：10/60：20：20
#交叉验证：在实际操作的过程中通常有一些数据集很小，这导致当我们按比例选取数据集中不同部分的数据作为训练集和验证集的时候导致训练的模型差异大，从而导致没办法得到最优的超参组合。提出交叉验证策略
  #所以我们按比例（eg.4折交叉/k折交叉验证）取数据集中不同部分数据作为验证集来计算不同模型中某一模型的得分，然后将所有得分取平均值将其作为模型得分

    # #################   Convert training data and compute conversion factors    ######################  接下来的代码用于转换训练数据并计算转换因子，为后续的数据处理做准备

    # all 47 columns of interest
    colbin = ['gender','mechvent','max_dose_vaso','re_admission'] 存储需要进行二值化处理的列名，这些列的数据可能只有两种取值，例如性别（男 / 女）等
    colnorm= ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance'] 
    #存储需要进行归一化处理的列名，这些列的数据通常是连续的数值，归一化可以将数据缩放到一个特定的范围，便于模型处理
    collog=['SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total','input_4hourly','output_total','output_4hourly']
    #存储需要进行对数变换的列名，对数变换可以将数据的分布进行调整，使其更符合模型的要求

    colbin=np.where(np.isin(MIMICtable.columns,colbin))[0]  #np.where(np.isin(MIMICtable.columns, colbin))[0]：取元组中的第一个元素，即得到 MIMICtable 数据集中属于 colbin 列表的列的【索引】
    colnorm=np.where(np.isin(MIMICtable.columns,colnorm))[0]  #同上
    collog=np.where(np.isin(MIMICtable.columns,collog))[0]  #同上

    # find patients who died in ICU during data collection period  #找出在数据收集期间于 ICU 去世的患者。具体筛选条件为：bloc 等于 1、died_within_48h_of_out_time 等于 1 且 delay_end_of_record_and_discharge_or_death 小于 24
    # ii=MIMICtable.bloc==1&MIMICtable.died_within_48h_of_out_time==1& MIMICtable.delay_end_of_record_and_discharge_or_death<24;
    # icustayidlist=MIMICtable.icustayid;
    # ikeep=~ismember(icustayidlist,MIMICtable.icustayid(ii));
    reformat5=MIMICtable.values.copy()  #把 MIMICtable 的数据值复制到 reformat5 中
    # reformat5=reformat5(ikeep,:);  
    icustayidlist=MIMICtable['icustayid']     #从 MIMICtable 里提取 icustayid 列的数据 (什么数据？)
    icuuniqueids=np.unique(icustayidlist) # list of unique icustayids from MIMIC  获取 icustayid 列中的唯一值，也就是所有不同的患者 ICU 停留 ID
    idxs=np.full((icustayidlist.shape[0],nr_reps),np.nan) # record state membership test cohort   创建一个形状为 (icustayidlist.shape[0], nr_reps) 的数组 idxs，并将其元素初始化为 NaN，该数组用于记录测试队列的状态成员信息

    MIMICraw=MIMICtable.iloc[:, np.concatenate([colbin,colnorm,collog])]   #从 MIMICtable 中提取 colbin、colnorm 和 collog 所指定列的数据
    MIMICraw=MIMICraw.values.copy()  # RAW values  把提取的数据值复制到 MIMICraw 中，得到原始数据
    MIMICzs=np.concatenate([reformat5[:, colbin]-0.5, zscore(reformat5[:,colnorm],ddof=1), zscore(np.log(0.1+reformat5[:, collog]),ddof=1)],axis=1)
    #对不同类型的列进行不同的处理后，再将结果按列拼接成一个新的数组 MIMICzs。具体处理方式为：colbin 列的数据减去 0.5；colnorm 列的数据使用 zscore 函数进行归一化处理；collog 列的数据先加 0.1 后取对数，再使用 zscore 函数进行归一化处理
    MIMICzs[:,3]=np.log(MIMICzs[:,3]+0.6)   # MAX DOSE NORAD 对 MIMICzs 数组的第 3 列数据加 0.6 后取对数。
    MIMICzs[:,44]=2*MIMICzs[:,44]   # increase weight of this variable  将 MIMICzs 数组的第 44 列数据乘以 2，以此增加该变量的权重


    # eICU section was not implemented 
    
    # compute conversion factors using MIMIC data 计算MIMIC转换因子
    a=MIMICraw[:, 0:3]-0.5  #从 MIMICraw数据集中提取前三列的所有数据，将每个元素减去0.5，最终结果储存在变量a中
    b= np.log(MIMICraw[:,3]+0.1)   #从MIMICraw 数据集中选取第 4 列（索引为 3）的所有数据，将所有元素加0.1为了避免对数运算中出现取零的对数，处理后将shujv取对数
    c,cmu,csigma = my_zscore(MIMICraw[:,4:36])   #从 MIMICraw 数据集中选取第 5 列（索引为 4）到第 36 列的所有行数据，用my_zscore函数最选取的数据进行标准化处理，将数据转化为均值为0，标准差为1的标准正态分布；函数返回三个值：c标准化后的数据/cmu原始数据的均值/csigma原始数据的标准差
    d,dmu,dsigma = my_zscore(np.log(0.1+MIMICraw[:,36:47]))   #从数据集中第37列到47列的所有行数取出，将每个元素加上0.1后取对数，然后调用my_zscore函数标准化处理，返回三个值标准化后的数据 d、原始数据的均值 dmu 和标准差 dsigma
    

    ####################### Main LOOP ###########################    主循环
    bestpol = 0 
    
    for modl in range(nr_reps):  # MAIN LOOP OVER ALL MODELS 所有模型遍历500个
        N=icuuniqueids.size # total number of rows to choose from  #获取 icuuniqueids 的元素数量，即患者 ID 的总数
        grp=np.floor(ncv*np.random.rand(N,1)+1);  #list of 1 to 5 (20% of the data in each grp) -- this means that train/test MIMIC split are DIFFERENT in all the 500 models  生成一个长度为 N 的数组 grp，其中每个元素是 1 到 ncv 之间的随机整数，用于将数据划分为 ncv 个组，这里 ncv 表示交叉验证的折数
        crossval=1;  #不懂，这个变量前面没有啊，这是干嘛的？交叉验证的折，意会一下，就是那个验证集的占比1（：4）
        trainidx=icuuniqueids[np.where(grp!=crossval)[0]]  #根据 grp 数组将患者 ID 划分为训练集和测试集
        testidx=icuuniqueids[np.where(grp==crossval)[0]]
        train=np.isin(icustayidlist,trainidx)  #生成布尔数组 train 和 test，用于标记每个样本是否属于训练集或测试集
        test=np.isin(icustayidlist,testidx)
        X=MIMICzs[train,:] #根据 train 数组将 MIMICzs 数据集划分为训练集 X 和测试集 Xtestmimic
        Xtestmimic=MIMICzs[~train,:]
        blocs=reformat5[train,0]  #根据 train 数组将 reformat5 数据集的第一列划分为训练集 blocs 和测试集 bloctestmimic
        bloctestmimic=reformat5[~train,0]
        ptid=reformat5[train,1]  #根据 train 数组将 reformat5 数据集的第二列划分为训练集 ptid 和测试集 ptidtestmimic
        ptidtestmimic=reformat5[~train,1] 
        outcome=9 #   HOSP _ MORTALITY = 7 / 90d MORTA = 9 设置要预测的结果列索引为 9，90天死亡率
        Y90=reformat5[train,outcome];   #从训练集中提取要预测的结果列数据


        print('########################   MODEL NUMBER : ',modl)   #打印当前模型的编号和当前时间，方便监控模型训练的进度
        print(datetime.datetime.now())

        #######   find best clustering solution (lowest intracluster variability)  ####################  寻找最佳聚类解决方案（集群内变异性最低）
        print('####  CLUSTERING  ####') # BY SAMPLING 输出“聚类”
        N=X.shape[0] #total number of rows to choose from   #获取训练集 X 的样本数量
        sampl=X[np.where(np.floor(np.random.rand(N,1)+prop))[0],:]  #从训练集 X 中随机采样一部分数据作为聚类的输入，采样比例由 prop 决定

        C = kmeans_with_multiple_runs(ncl,10000,nclustering,sampl)  #调用 kmeans_with_multiple_runs 函数对采样数据进行多次 K-Means 聚类，返回最优的聚类模型 C
        idx = C.predict(X)  #使用最优的聚类模型 C 对整个训练集 X 进行聚类预测，得到每个样本所属的聚类编号

        ############################## CREATE ACTIONS  ########################   创建动作
        print('####  CREATE ACTIONS  ####') 

        nact=nra*nra #动作总数？

        iol=MIMICtable.columns.get_loc('input_4hourly')  #获取 MIMICtable 数据集中 input_4hourly 列的索引，可能表示每四小时静脉输液量？
        vcl=MIMICtable.columns.get_loc('max_dose_vaso')  #获取 MIMICtable 数据集中 max_dose_vaso 列的索引，血管活性药物最大剂量？

        a= reformat5[:,iol].copy()                   # IV fluid 复制 reformat5 数据集中 input_4hourly 列的数据到 a 中
        a= rankdata(a[a>0])/a[a>0].shape[0]   # excludes zero fluid (will be action 1)  rankdata(a[a > 0]) 会对 a 中所有大于 0 的元素进行排名，并返回一个包含排名的数组；a[a > 0]是一个布尔索引操作，它从数组 a 中筛选出所有大于 0 的元素，a[a > 0].shape[0] 表示 a 中大于 0 的元素的数量；将排名数组除以这个数量，就实现了对排名的归一化处理，将排名值缩放到 [0, 1] 区间内；其中0值作为一个输液量被设置为动作1

        iof=np.floor((a+0.2499999999)*4)  #converts iv volume in 4 actions  #将数组a中数据处理（+0.249999之后再乘4）后使用floor 函数对前面计算得到的数组进行向下取整操作将最终的结果赋值给变量 iof。操作最后将输液量划分为4个不同类别或者等级，定为4个动作

        a= reformat5[:,iol].copy()  #再次复制 reformat5 数据集中 input_4hourly 列的数据到 a 中？不会影响吗
        a= np.where(a>0)[0]  # location of non-zero fluid in big matrix  #找出非零数据的索引

        io=np.ones((reformat5.shape[0],1))  # array of ones, by default   创建一个元组，这里表示行数为 reformat5 的行数，列数为 1；使用 numpy 的 ones 函数创建一个指定形状的数组，数组中的所有元素都初始化为 1 
        io[a]=(iof+1).reshape(-1,1)   # where more than zero fluid given: save actual action  此语句是为了在静脉输液量大于 0 的位置，保存实际的动作编号； iof是一个保存动作编号的数组，a是非零输液量数据的索引，【(iof + 1).reshape(-1, 1)：将 iof + 1 数组重新调整形状为列向量，-1 表示该维度的大小由数组的总元素数和其他维度的大小自动计算得出？不懂】
        io = io.ravel()  #使用 numpy 的 ravel 函数将 io 数组展平为一维数组。ravel 函数会返回一个一维数组，包含 io 数组中的所有元素，并重新赋值给io
        #以上 io 数组就记录了每个样本对应的动作编号，其中默认值为 1，静脉输液量大于 0 的位置保存了实际的动作编号


        vc=reformat5[:,vcl].copy() 
        vcr= rankdata(vc[vc!=0])/vc[vc!=0].size
        vcr=np.floor((vcr+0.249999999999)*4)  # converts to 4 bins
        vcr[vcr==0]=1
        vc[vc!=0]=vcr+1 
        vc[vc==0]=1

        ma1 = np.array([np.median(reformat5[io==1,iol]),np.median(reformat5[io==2,iol]),np.median(reformat5[io==3,iol]), np.median(reformat5[io==4,iol]),np.median(reformat5[io==5,iol])]) # median dose of drug in all bins
        ma2 = np.array([np.median(reformat5[vc==1,vcl]),np.median(reformat5[vc==2,vcl]),np.median(reformat5[vc==3,vcl]), np.median(reformat5[vc==4,vcl]),np.median(reformat5[vc==5,vcl])])

        med = np.concatenate([io.reshape(-1,1),vc.reshape(-1,1)],axis=1)
        uniqueValues,actionbloc = np.unique(med,axis=0,return_inverse=True)

        actionbloctrain=actionbloc[train] 

        ma2Values = ma2[uniqueValues[:,1].astype('int64')-1].reshape(-1,1)
        ma1Values = ma1[uniqueValues[:,0].astype('int64')-1].reshape(-1,1)

        uniqueValuesdose = np.concatenate([ma2Values,ma1Values],axis=1) # median dose of each bin for all 25 actions 

        ####################################################################################################################################
        print('####  CREATE QLDATA3  ####')

        r=np.array([100, -100]).reshape(1,-1)
        r2=r*(2*(1-Y90.reshape(-1,1))-1)
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)
        qldata=np.concatenate([blocs.reshape(-1,1), idx.reshape(-1,1), actionbloctrain.reshape(-1,1), Y90.reshape(-1,1), r2],axis=1)  # contains bloc / state / action / outcome&reward     
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),4))
        c=-1
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 

        for i in range(qldata.shape[0]-1):
            c=c+1
            qldata3[c,:]=qldata[i,0:4] 
            if(qldata[i+1,0]==1): #end of trace for this patient
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,4]]) 

        qldata3=qldata3[:c+1,:]


        # ###################################################################################################################################
        print("####  CREATE TRANSITION MATRIX T(S'',S,A) ####")
        transitionr=np.zeros((ncl+2,ncl+2,nact))  #this is T(S',S,A)
        sums0a0=np.zeros((ncl+2,nact)) 

        for i in range(qldata3.shape[0]-1):    
            if (qldata3[i+1,0]!=1) : # if we are not in the last state for this patient = if there is a transition to make!
                S0=int(qldata3[i,1]) 
                S1=int(qldata3[i+1,1])
                acid= int(qldata3[i,2]) 
                transitionr[S1,S0,acid]=transitionr[S1,S0,acid]+1 
                sums0a0[S0,acid]=sums0a0[S0,acid]+1

        sums0a0[sums0a0<=transthres]=0  #delete rare transitions (those seen less than 5 times = bottom 50%!!)

        for i in range(ncl+2): 
            for j in range(nact): 
                if sums0a0[i,j]==0: 
                    transitionr[:,i,j]=0; 
                else:
                    transitionr[:,i,j]=transitionr[:,i,j]/sums0a0[i,j]


        transitionr[np.isnan(transitionr)]=0  #replace NANs with zeros
        transitionr[np.isinf(transitionr)]=0  #replace NANs with zeros

        physpol=sums0a0/np.sum(sums0a0, axis=1).reshape(-1,1)    #physicians policy: what action was chosen in each state


        print("####  CREATE TRANSITION MATRIX T(S,S'',A)  ####")

        transitionr2=np.zeros((ncl+2,ncl+2,nact))  # this is T(S,S',A)
        sums0a0=np.zeros((ncl+2,nact))

        for i in range(qldata3.shape[0]-1) : 
            if (qldata3[i+1,0]!=1) : # if we are not in the last state for this patient = if there is a transition to make!
                S0=int(qldata3[i,1])
                S1=int(qldata3[i+1,1])
                acid= int(qldata3[i,2]) 
                transitionr2[S0,S1,acid]=transitionr2[S0,S1,acid]+1;  
                sums0a0[S0,acid]=sums0a0[S0,acid]+1

        sums0a0[sums0a0<=transthres]=0;  #delete rare transitions (those seen less than 5 times = bottom 50%!!) IQR = 2-17

        for i in range(ncl+2): 
            for j in range(nact): 
                if sums0a0[i,j]==0:
                    transitionr2[i,:,j]=0 
                else: 
                    transitionr2[i,:,j]=transitionr2[i,:,j]/sums0a0[i,j]

        transitionr2[np.isnan(transitionr2)]=0 #replace NANs with zeros
        transitionr2[np.isinf(transitionr2)]=0 # replace infs with zeros

        print('####  CREATE REWARD MATRIX  R(S,A) ####')
        # CF sutton& barto bottom 1998 page 106. i compute R(S,A) from R(S'SA) and T(S'SA)
        r3=np.zeros((ncl+2,ncl+2,nact))
        r3[ncl,:,:]=-100
        r3[ncl+1,:,:]=100
        R=sum(transitionr*r3)
        R=np.squeeze(R)   #remove 1 unused dimension


        print('####  POLICY ITERATION   ####')


        pi = PolicyIteration_with_Q(transitionr2, R, gamma, np.ones((ncl+2,1))) 
        Qon = np.transpose(pi.run()) 
        OptimalAction=np.argmax(Qon,axis=1).reshape(-1,1)  #deterministic 
        OA[:,modl]=OptimalAction.ravel() #save optimal actions


        print('####  OFF-POLICY EVALUATION - MIMIC TRAIN SET ####')
        # create new version of QLDATA3

        r=np.array([100, -100]).reshape(1,-1)
        r2=r*(2*(1-Y90.reshape(-1,1))-1)
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)
        qldata=np.concatenate([blocs.reshape(-1,1), idx.reshape(-1,1), actionbloctrain.reshape(-1,1), Y90.reshape(-1,1),np.zeros((idx.size,1)), r2[:,0].reshape(-1,1), ptid.reshape(-1,1) ],axis=1)   # contains bloc / state / action / outcome&reward     
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),8))
        c=-1
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 

        for i in range(qldata.shape[0]-1):
            c=c+1
            qldata3[c,:]=qldata[i,[0,1,2,4,6,6,6,6]] 
            if(qldata[i+1,0]==1): #end of trace for this patient
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,5],0,0,-1,qldata[i,6]]) 

        qldata3=qldata3[:c+1,:]

        #  add pi(s,a) and b(s,a)
        p=0.01 #softening policies 
        softpi=physpol.copy() # behavior policy = clinicians' 
        for i in range(ncl): 
            ii=softpi[i,:]==0    
            z=p/sum(ii)    
            nz=p/sum(~ii)    
            softpi[i,ii]=z;   
            softpi[i,~ii]=softpi[i,~ii]-nz;

        softb=np.abs(np.zeros((ncl+2,nact))-p/24) #"optimal" policy = target policy = evaluation policy 

        for i in range(ncl): 
            softb[i,OptimalAction[i]]=1-p


        for i in range(qldata3.shape[0]):  # adding the probas of policies to qldata3
            if qldata3[i,1]<ncl :
                qldata3[i,4]=softpi[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,5]=softb[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,6]=OptimalAction[int(qldata3[i,1])]  #optimal action

        qldata3train=qldata3.copy() 


        bootql,bootwis = offpolicy_multiple_eval_010518(qldata3,physpol, 0.99,1,6,750)

        recqvi[modl,0]=modl
        recqvi[modl,3]=np.nanmean(bootql)
        recqvi[modl,4]=mquantiles(bootql,0.99, alphap=0.5, betap=0.5)[0]
        recqvi[modl,5]=np.nanmean(bootwis) # we want this as high as possible
        recqvi[modl,6]=mquantiles(bootwis,0.05, alphap=0.5, betap=0.5)[0] #we want this as high as possible


        # testing on MIMIC-test
        print('####  OFF-POLICY EVALUATION - MIMIC TEST SET ####')

        # create new version of QLDATA3 with MIMIC TEST samples

        idxtest = C.predict(Xtestmimic)

        idxs[test,modl]=idxtest.ravel()  #important: record state membership of test cohort

        actionbloctest=actionbloc[~train] 
        Y90test=reformat5[~train,outcome]

        r=np.array([100, -100]).reshape(1,-1)
        r2=r*(2*(1-Y90test.reshape(-1,1))-1)
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)

        qldata=np.concatenate([bloctestmimic.reshape(-1,1), idxtest.reshape(-1,1), actionbloctest.reshape(-1,1), Y90test.reshape(-1,1),np.zeros((idxtest.size,1)), r2[:,0].reshape(-1,1), ptidtestmimic.reshape(-1,1) ],axis=1)   # contains bloc / state / action / outcome&reward     
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),8))
        c=-1
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 

        for i in range(qldata.shape[0]-1):
            c=c+1
            qldata3[c,:]=qldata[i,[0,1,2,4,6,6,6,6]] 
            if(qldata[i+1,0]==1): #end of trace for this patient
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,5],0,0,-1,qldata[i,6]]) 

        qldata3=qldata3[:c+1,:]

        #  add pi(s,a) and b(s,a)
        p=0.01 # small correction factor #softening policies 
        softpi=physpol.copy() # behavior policy = clinicians' 
        for i in range(ncl): 
            ii=softpi[i,:]==0    
            z=p/sum(ii)    
            nz=p/sum(~ii)    
            softpi[i,ii]=z;   
            softpi[i,~ii]=softpi[i,~ii]-nz;

        softb=np.abs(np.zeros((ncl+2,nact))-p/24) #"optimal" policy = target policy = evaluation policy 

        for i in range(ncl): 
            softb[i,OptimalAction[i]]=1-p

        for i in range(qldata3.shape[0]):  # adding the probas of policies to qldata3
            if qldata3[i,1]<ncl :
                qldata3[i,4]=softpi[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,5]=softb[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,6]=OptimalAction[int(qldata3[i,1])]  #optimal action

        qldata3test=qldata3.copy() 

        bootmimictestql,bootmimictestwis = offpolicy_multiple_eval_010518(qldata3,physpol, 0.99,1,6,2000)

        recqvi[modl,18]=mquantiles(bootmimictestql,0.95, alphap=0.5, betap=0.5)[0] #PHYSICIANS' 95% UB
        recqvi[modl,19]=np.nanmean(bootmimictestql)
        recqvi[modl,20]=mquantiles(bootmimictestql,0.99, alphap=0.5, betap=0.5)[0]
        recqvi[modl,21]=np.nanmean(bootmimictestwis) 
        recqvi[modl,22]=mquantiles(bootmimictestwis,0.01, alphap=0.5, betap=0.5)[0] 
        recqvi[modl,23]=mquantiles(bootmimictestwis,0.05, alphap=0.5, betap=0.5)[0] #AI 95% LB, we want this as high as possible




        if recqvi[modl,23] > 40: #saves time if policy is not good on MIMIC test: skips to next model
            print('########################## eICU TEST SET #############################')
            # eICU part was not implemented 



        # eICU testing was not included 
        if recqvi[modl,23]>0 : #  & recqvi(modl,14)>0   # if 95% LB is >0 : save the model (otherwise it's pointless)
            print('####   GOOD MODEL FOUND - SAVING IT   ####' ) 

            # best pol 
            if(bestpol < recqvi[modl,23]):
                print('Best policy was replaced => 95% LB is ',recqvi[modl,23])
                bestpol = recqvi[modl,23] 
                
                # save to pickle 
                with open('bestpol.pkl', 'wb') as file:
                    pickle.dump(modl,file)
                    pickle.dump(Qon,file)
                    pickle.dump(physpol,file)
                    pickle.dump(transitionr,file) 
                    pickle.dump(transitionr2,file)
                    pickle.dump(R,file)
                    pickle.dump(C,file)
                    pickle.dump(train,file)
                    pickle.dump(qldata3train,file)
                    pickle.dump(qldata3test,file) 


    recqvi=recqvi[:modl+1,:]

    # save to pickle for visualization 
    with open('step_5_start.pkl', 'wb') as file:
        pickle.dump(MIMICzs,file)
        pickle.dump(actionbloc,file)
        pickle.dump(reformat5,file)
        pickle.dump(recqvi,file)

    # save recqvi in csv format 
    np.savetxt('recqvi.csv',recqvi,delimiter=',') 



# In[ ]:




