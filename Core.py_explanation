#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# Re-implmentation of AI Clinician Matlab Code in Python 
# Author: KyungJoong Kim (GIST, South Korea)
# Date: 2020 June 2 
# 
# This code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE


# Note 
#
# K-Means in scikit-learn will produce differnt outcome with Matlab's original K-means 
# The random number generator will produce different random number sequence 
# 
# Todo: 
# eICU was not included in this re-implementation 





## AI Clinician core code

# (c) Matthieu Komorowski, Imperial College London 2015-2019
# as seen in publication: https://www.nature.com/articles/s41591-018-0213-5

# version 16 Feb 19
# Builds 500 models using MIMIC-III training data
# Records best candidate models along the way from off-policy policy evaluation on MIMIC-III validation data
# Tests the best model on eRI data


# TAKES:
        # MIMICtable = m*59 table with raw values from MIMIC
        # eICUtable = n*56 table with raw values from eICU
        

# GENERATES:
        # MIMICraw = MIMIC RAW DATA m*47 array with columns in right order
        # MIMICzs = MIMIC ZSCORED m*47 array with columns in right order, matching MIMICraw
        # eICUraw = eICU RAW DATA n*47 array with columns in right order, matching MIMICraw
        # eICUzs = eICU ZSCORED n*47 array with columns in right order, matching MIMICraw
        # recqvi = summary statistics of all 500 models
        # idxs = state membership of MIMIC test records, for all 500 models
     	# OA = optimal policy, for all 500 models
        # allpols = detailed data about the best candidate models

# This code is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY;
# without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE

# Note: The size of the cohort will depend on which version of MIMIC-III is used.
# The original cohort from the 2018 Nature Medicine publication was built using MIMIC-III v1.3.

import pickle #用于加载/保存python对象
import numpy as np #库可以用于处理多维数组和矩阵运算
import pandas as pd #基于numpy的第三方数据分析库，提供高效数据结构以及数据分析工具
from scipy.stats import zscore, rankdata #scipy库提供优化，积分，差值，统计等多种算法。本语句是从scipy.stats子模块中导入zscore和rankdata两个函数；zscore函数是计算数组中各元素的z分数，表示某个数据点距离均值有多少标准差；rankdata用于计算数组中各元素排名/平均排名/最小排名等。
import math #数学类函数库，仅支持整数和浮点数运算
import scipy.io as sio #语句从scipy库中导入io子模块，并为这个子模块指定一个别名sio；scipy.io可以处理不同格式的数据文件如MATLAB文件，图像文件等；别名方便后续代码书写
import datetime #datetime库可以处理日期和时间
from scipy.stats.mstats import mquantiles #scipy.stats 是 SciPy 库中用于统计分析的子模块，而 mstats 是 stats 里专门处理掩码数组（masked arrays）的子模块（掩码数组是一种特殊的数组，它允许你标记某些元素为无效值，在进行统计计算时可以【忽略这些无效值】）；mquantiles函数用于计算掩码数组的分位数（eg，50百分位数）
from mdptoolbox.mdp import PolicyIteration #此语句的作用是从 mdptoolbox 库的 mdp 子模块里导入PolicyIteration类；mdptoolbox是一个用于解决马尔可夫决策过程（Markov Decision Process, MDP）问题的库；PolicyIteration可以实现策略迭代算法
from reinforcement_learning_mp import offpolicy_multiple_eval_010518 #从文件中导入数据？
from kmeans_mp import kmeans_with_multiple_runs #k-means算法模块
from multiprocessing import freeze_support #freeze_support 函数主要用于支持在 Windows 系统以及其他不支持 fork 系统调用的平台上，将使用 multiprocessing 模块编写的 Python 程序打包成可执行文件（如使用 PyInstaller、cx_Freeze 等工具打包）；在这些平台上，如果不使用 freeze_support，打包后的程序在运行时可能会出现各种问题，比如程序崩溃、无限创建子进程等

def my_zscore(x):  #定义一个my_zscore的函数，x为自变量
    return zscore(x,ddof=1),np.mean(x,axis=0),np.std(x,axis=0,ddof=1) #返回值三个值：数组x经过z分数标准化后的结果（ddof=1表示计算标准差时自由度为1）、数组x沿指定轴的均值（axis=0通常是计算列均值），数组x沿指定轴的标准差


# In[ ]:


######### Functions used in Reinforcement Learning ########   强化学习中用到的函数


class PolicyIteration_with_Q(PolicyIteration): #定义一个子类PolicyIteration_with_Q，继承自父类PolicyIteration；继承意味着 PolicyIteration_with_Q 类会自动拥有 PolicyIteration 类的所有属性和方法，同时还可以添加或修改自己特有的属性和方法
    def __init__(self, transitions, reward, discount, policy0=None,max_iter=1000, eval_type=0, skip_check=False):  #定义类中的属性，transitions表示状态转移概率矩阵；reward为奖励矩阵；discount折扣因子（即γ）；policy0初始策略默认为none；max_iter最大迭代次数默认为1000，为迭代终止条件；eval_type评估类型可能用于指定策略评估阶段的具体方法？默认为0；skip_check是否跳过检查，默认为false，可能用于控制是否对输入的transitions和reward矩阵进行有效性检查
        # Python MDP toolbox from https://github.com/sawcordwell/pymdptoolbox
        # In Matlab MDP Toolbox, P = (S, S, A), R = (S, A) 
        # In Python MDP Toolbox, P = (A, S, S), R= (S, A)

        transitions = np.transpose(transitions,(2,0,1)).copy() # Change to Action First (A, S, S) #np.transpose 是一个函数，用于对数组的维度进行转置操作；它接受两个参数，第一个参数是要进行转置的数组，第二个参数是一个元组，用于指定新的维度顺序；因为原有数组顺序为（S,S,A）,转换为(A,S,S)；（2，0，1）表示新的维度数据，原数组的第二个维度会变成新数组的第0个维度，原数组的第0个维度会变成新数组的第个维度...；对 transitions 数组进行维度转置操作，并且将结果复制一份重新赋值给transitions变量，copy方法是为了创建一个转制后的副本，使用copy可以确保后续对transitions数组的修改不会影响到原数组
        skip_check = True # To Avoid StochasticError: 'PyMDPToolbox - The transition probability matrix is not stochastic.'  在马尔可夫决策过程（MDP）中，转移概率矩阵需要满足随机矩阵的性质，即矩阵的每一行元素之和必须等于 1；PyMDPToolbox 库在进行某些操作时会检查转移概率矩阵是否为随机矩阵，如果不满足条件，就会抛出 StochasticError 异常；将 skip_check 设置为 True 后，可能会跳过对转移概率矩阵的随机矩阵检查，从而避免因矩阵不满足随机矩阵条件而抛出异常；但跳过检查可能会导致后续的计算结果不准确，因为 MDP 算法通常假设转移概率矩阵是随机矩阵

        PolicyIteration.__init__(self, transitions, reward, discount, policy0=None,max_iter=1000, eval_type=0, skip_check=skip_check) #父类自身初始化，将父类初始化放在子类之后可以避免覆盖父类初始化属性的情况发生
    
    def _bellmanOperator_with_Q(self, V=None): #定义一个方法名为 _bellmanOperator_with_Q，且名字前有_表明方法主要用于类内部的实现。V初始值为None，若无V值传入则为默认值，若有则为传入值
        # Apply the Bellman operator on the value function.
        #
        # Updates the value function and the Vprev-improving policy.
        #
        # Returns: (policy, Q, value), tuple of new policy and its value  方法会返回一个元组 (policy, Q, value)，其中 policy 是新的策略，Q 通常代表动作价值函数（Q-function），value 是新策略对应的值函数
        #
        # If V hasn't been sent into the method, then we assume to be working    如果 V 没有被传入方法，那么方法会假设使用类实例对象自身的 V 属性进行计算，即使用类中已经存储的值函数进行贝尔曼算子的应用
        # on the objects V attribute
        if V is None:   #下面这段代码的作用是在 V 为 None 时使用类实例的 V 属性，在 V 不为 None 时检查其是否为 numpy 数组或矩阵，以及形状是否符合要求。
            # this V should be a reference to the data rather than a copy
            V = self.V
        else:
            # make sure the user supplied V is of the right shape
            try:
                assert V.shape in ((self.S,), (1, self.S)), "V is not the "                     "right shape (Bellman operator)."  #检查 V 的形状是否符合要求。self.S 通常代表状态的数量。((self.S,), (1, self.S)) 表示 V 可以是一维数组（形状为 (self.S,)），也可以是二维数组且只有一行（形状为 (1, self.S)）。如果 V 的形状不在这两种情况之内，assert 语句会触发 AssertionError，并输出错误信息 "V is not the right shape (Bellman operator)."
            except AttributeError:  #捕获 AttributeError 异常
                raise TypeError("V must be a numpy array or matrix.")  #当捕获到该异常时，raise TypeError("V must be a numpy array or matrix.") 会抛出一个 TypeError 异常，提示用户 V 必须是 numpy 数组或矩阵
        # Looping through each action the the Q-value matrix is calculated.    代码会遍历每一个动作，进而计算 Q 值矩阵。在马尔可夫决策过程（MDP）中，Q 值表示在某个状态下采取某个动作所能获得的期望累积奖励
        # P and V can be any object that supports indexing, so it is important
        # that you know they define a valid MDP before calling the
        # _bellmanOperator method. Otherwise the results will be meaningless.
        Q = np.empty((self.A, self.S))  #使用 np.empty 函数创建一个形状为 (self.A, self.S) 的空数组 Q，其中 self.A 代表动作的数量，self.S 代表状态的数量。这个数组将用于存储计算得到的 Q 值矩阵
        for aa in range(self.A):  #通过 for 循环遍历每一个动作 aa
            Q[aa] = self.R[aa] + self.discount * self.P[aa].dot(V) #对于每个动作 aa，根据贝尔曼方程计算 Q 值。self.R[aa] 表示在采取动作 aa 时的即时奖励，self.discount 是折扣因子，self.P[aa].dot(V) 表示在采取动作 aa 后，根据状态转移概率矩阵 self.P[aa] 对下一个状态的价值函数 V 进行加权求和。将即时奖励和折扣后的下一个状态的价值相加，得到在该状态下采取该动作的 Q 值
        # Get the policy and value, for now it is being returned but...
        # Which way is better?
        # 1. Return, (policy, value)
        return (Q.argmax(axis=0), Q, Q.max(axis=0)) #直接返回一个元组 (Q.argmax(axis=0), Q, Q.max(axis=0))。Q.argmax(axis=0) 表示在每个状态下选择 Q 值最大的动作，得到新的策略；Q 是计算得到的 Q 值矩阵；Q.max(axis=0) 表示在每个状态下 Q 值的最大值，得到新的价值函数
        # 2. update self.policy and self.V directly
        # self.V = Q.max(axis=1)
        # self.policy = Q.argmax(axis=1)
    
    def run(self):   #到167行这段定义了一个方法run，用来实现策略迭代算法
        # Run the policy iteration algorithm.
        self._startRun()  #可能在父类中？/某个已经定义好的类中的方法函数

        while True:   #创建无限循环
            self.iter += 1  #每迭代一次self.iter加1
            # these _evalPolicy* functions will update the classes value
            # attribute
            if self.eval_type == "matrix":  #根据self.eval_type的值选择不同的策略评估方法，若为matrix则调用self._evalPolicyMatrix方法（可能是矩阵计算？）
                self._evalPolicyMatrix()
            elif self.eval_type == "iterative":  #若为iterative则调用self._evalPolicyIterative方法（可能是迭代的方式计算？）
                self._evalPolicyIterative()
            # This should update the classes policy attribute but leave the
            # value alone
            policy_next, Q, null = self._bellmanOperator_with_Q()  #调用self._bellmanOperator_with_Q方法，该方法会返回新的策略policy_next, Q矩阵和null值
            del null  #删除null变量，释放内存
            # calculate in how many places does the old policy disagree with  #计算新旧策略（policy_next与self.policy）在多少个状态上不一样，赋值给n_different
            # the new policy
            n_different = (policy_next != self.policy).sum()  #计算策略间差异
            # if verbose then continue printing a table
            if self.verbose:  #verbose是一个控制输出的标志变量，设为true可以输出详细的运行信息排查问题，设为false则可以避免不必要的输出
                _printVerbosity(self.iter, n_different)  #如果self.verbose为True，则调用 _printVerbosity 方法，输出当前迭代次数和策略差异的详细信息
            # Once the policy is unchanging of the maximum number of
            # of iterations has been reached then stop
            if n_different == 0:   #以下为终止条件判断 如果n_different=0则说明新旧策略完全相同策略已经收敛，此时跳出循环
                if self.verbose:  #输出对应的文字policy unchanging. Stopping iteration.
                    print(_MSG_STOP_UNCHANGING_POLICY)
                break
            elif self.iter == self.max_iter:  或者self.iter等于self.max_iter说明达到预设的最大迭代次数，跳出循环并打印Maximum number of iterations reached. Stopping optimization
                if self.verbose:
                    print(_MSG_STOP_MAX_ITER)
                break
            elif self.iter > 20 and n_different <=5 : # This condition was added from the Nature Code   #如果迭代次数大于20且策略差异小于5，也停止迭代并打印Training stopped early
                if self.verbose: 
                    print((_MSG_STOP))  #多一层括号？
                break 
            else:  #如果不符合上面的终止条件将policy_next赋值给self.policy继续迭代
                self.policy = policy_next 

        self._endRun()  调用_endrun方法（可能用于清理资源？）
        
        return Q  #返回最终得到的Q值
    


# In[ ]:
#174-183行主要实现了在主程序中加载step_4_start.pkl文件中数据，同时处理打包和警告过滤相关的问题

if __name__ == '__main__':   #确保代码块里的代码只有在该 Python 文件被直接运行时才会执行，若作为模块被导入则不会执行，这样可以（1）防止不必要的执行，通常导入模块时只希望导入模块的功能（2）大型项目中通常有一个主程序文件作为程序的入口点，可以明确哪些代码是程序起始执行部分？
    
    freeze_support()  #freeze_support函数主要用于支持将 Python 脚本打包成可执行文件，没有导入multiprocessing模块？最前面有
    
    # To ignore 'Runtime Warning: Invalid value encountered in greater' caused by NaN 
    np.warnings.filterwarnings('ignore')  #为了忽略 'Runtime Warning: Invalid value encountered in greater' 警告

    # Load pickle 
    with open('step_4_start.pkl', 'rb') as file:  #使用 with 语句打开一个二进制文件 step_4_start.pkl，'rb' 表示以二进制只读模式打开文件。with 语句会自动管理文件的打开和关闭，保证文件操作结束后会正确关闭文件
        MIMICtable = pickle.load(file)   #使用 pickle 模块的 load 函数从打开的文件中加载数据，并将其赋值给变量 MIMICtable
    
    #############################  MODEL PARAMETERS   #####################################   模型参数设置

    print('####  INITIALISATION  ####')    #提示进行初始化操作

    nr_reps=500               # nr of repetitions (total nr models) % 500   总共要训练的模型数量，500
    nclustering=32            # how many times we do clustering (best solution will be chosen) % 32   表示聚类操作的次数，程序会从聚类结果中选择最优的解决方案，设为32
    prop=0.25                 # proportion of the data we sample for clustering    用于聚类的数据采样比例，设置为0.25
    gamma=0.99                # gamma  折扣因子
    transthres=5              # threshold for pruning the transition matrix   用于修剪转移矩阵的阈值，当转移矩阵中的某些元素满足特定条件（可能是小于该阈值）时，会被修剪掉
    polkeep=1                 # count of saved policies   保存的策略数量，这里设置为 1，表示只保存一个最优策略
    ncl=750                   # nr of states   状态的数量，设置为 750
    nra=5                     # nr of actions (2 to 10)   ？这是哪个参数
    ncv=5                     # nr of crossvalidation runs (each is 80% training / 20% test)   #交叉验证的运行次数，每次交叉验证会将数据划分为 80% 的训练集和 20% 的测试集，这里设置为 5
    OA=np.full((752,nr_reps),np.nan)       # record of optimal actions  使用 numpy 的 full 函数创建一个形状为 (752, nr_reps) 的二维数组，初始值都为 NaN（非数字），用于记录最优动作
    recqvi=np.full((nr_reps*2,30),np.nan)  # saves data about each model (1 row per model)  使用 numpy 的 full 函数创建一个形状为 (nr_reps*2, 30) 的二维数组，初始值为 NaN，用于保存每个模型的相关数据，每行代表一个模型
    # allpols=[]  # saving best candidate models

#插入关于k_means聚类的介绍，文中是通过聚类算法将患者数据聚类为750个离散状态，并通过主成分分析来验证状态分布与死亡率之间的关系；聚类本身数据没有标签
  #k_means聚类是通过迭代的方式：先设置k个质心，数据点依据当前质心被分到某聚类中，再根据当前数据点重新计算聚类的质心
  #输入结果簇的个数k和包含n个对象的集合，输出是k个簇的集合
#插入关于数据集的划分问题：一部分数据用来训练模型让模型能够达到该有的功能以及预测效果，然后用测试集来验证模型的预测效果到底如何。通常两个数据集的比例为7：3或者8：2
  #但通常解决一个问题的算法有很多，模型也有很多，调整模型中的超参（参数的参数）也可以得到不同的模型参数，但哪一种模型最好？引入验证集，将训练集数据分为两部分，训练集和验证集，训练集用于训练模型，而验证集用于给不同模型打分，然后将两部分数据合在一起重新训练最后使用测试集来验证；通常使用的训练集验证集测试集的比例为70：15：15/80：10：10/60：20：20
#交叉验证：在实际操作的过程中通常有一些数据集很小，这导致当我们按比例选取数据集中不同部分的数据作为训练集和验证集的时候导致训练的模型差异大，从而导致没办法得到最优的超参组合。提出交叉验证策略
  #所以我们按比例（eg.4折交叉/k折交叉验证）取数据集中不同部分数据作为验证集来计算不同模型中某一模型的得分，然后将所有得分取平均值将其作为模型得分

    # #################   Convert training data and compute conversion factors    ######################  接下来的代码用于转换训练数据并计算转换因子，为后续的数据处理做准备

    # all 47 columns of interest
    colbin = ['gender','mechvent','max_dose_vaso','re_admission'] 存储需要进行二值化处理的列名，这些列的数据可能只有两种取值，例如性别（男 / 女）等
    colnorm= ['age','Weight_kg','GCS','HR','SysBP','MeanBP','DiaBP','RR','Temp_C','FiO2_1',        'Potassium','Sodium','Chloride','Glucose','Magnesium','Calcium',        'Hb','WBC_count','Platelets_count','PTT','PT','Arterial_pH','paO2','paCO2',        'Arterial_BE','HCO3','Arterial_lactate','SOFA','SIRS','Shock_Index','PaO2_FiO2','cumulated_balance'] 
    #存储需要进行归一化处理的列名，这些列的数据通常是连续的数值，归一化可以将数据缩放到一个特定的范围，便于模型处理
    collog=['SpO2','BUN','Creatinine','SGOT','SGPT','Total_bili','INR','input_total','input_4hourly','output_total','output_4hourly']
    #存储需要进行对数变换的列名，对数变换可以将数据的分布进行调整，使其更符合模型的要求

    colbin=np.where(np.isin(MIMICtable.columns,colbin))[0]  #np.where(np.isin(MIMICtable.columns, colbin))[0]：取元组中的第一个元素，即得到 MIMICtable 数据集中属于 colbin 列表的列的【索引】
    colnorm=np.where(np.isin(MIMICtable.columns,colnorm))[0]  #同上
    collog=np.where(np.isin(MIMICtable.columns,collog))[0]  #同上

    # find patients who died in ICU during data collection period  #找出在数据收集期间于 ICU 去世的患者。具体筛选条件为：bloc 等于 1、died_within_48h_of_out_time 等于 1 且 delay_end_of_record_and_discharge_or_death 小于 24
    # ii=MIMICtable.bloc==1&MIMICtable.died_within_48h_of_out_time==1& MIMICtable.delay_end_of_record_and_discharge_or_death<24;
    # icustayidlist=MIMICtable.icustayid;
    # ikeep=~ismember(icustayidlist,MIMICtable.icustayid(ii));
    reformat5=MIMICtable.values.copy()  #把 MIMICtable 的数据值复制到 reformat5 中
    # reformat5=reformat5(ikeep,:);  
    icustayidlist=MIMICtable['icustayid']     #从 MIMICtable 里提取 icustayid 列的数据 (什么数据？)
    icuuniqueids=np.unique(icustayidlist) # list of unique icustayids from MIMIC  获取 icustayid 列中的唯一值，也就是所有不同的患者 ICU 停留 ID
    idxs=np.full((icustayidlist.shape[0],nr_reps),np.nan) # record state membership test cohort   创建一个形状为 (icustayidlist.shape[0], nr_reps) 的数组 idxs，并将其元素初始化为 NaN，该数组用于记录测试队列的状态成员信息

    MIMICraw=MIMICtable.iloc[:, np.concatenate([colbin,colnorm,collog])]   #从 MIMICtable 中提取 colbin、colnorm 和 collog 所指定列的数据
    MIMICraw=MIMICraw.values.copy()  # RAW values  把提取的数据值复制到 MIMICraw 中，得到原始数据
    MIMICzs=np.concatenate([reformat5[:, colbin]-0.5, zscore(reformat5[:,colnorm],ddof=1), zscore(np.log(0.1+reformat5[:, collog]),ddof=1)],axis=1)
    #对不同类型的列进行不同的处理后，再将结果按列拼接成一个新的数组 MIMICzs。具体处理方式为：colbin 列的数据减去 0.5；colnorm 列的数据使用 zscore 函数进行归一化处理；collog 列的数据先加 0.1 后取对数，再使用 zscore 函数进行归一化处理
    MIMICzs[:,3]=np.log(MIMICzs[:,3]+0.6)   # MAX DOSE NORAD 对 MIMICzs 数组的第 3 列数据加 0.6 后取对数。
    MIMICzs[:,44]=2*MIMICzs[:,44]   # increase weight of this variable  将 MIMICzs 数组的第 44 列数据乘以 2，以此增加该变量的权重


    # eICU section was not implemented 
    
    # compute conversion factors using MIMIC data 计算MIMIC转换因子
    a=MIMICraw[:, 0:3]-0.5  #从 MIMICraw数据集中提取前三列的所有数据，将每个元素减去0.5，最终结果储存在变量a中
    b= np.log(MIMICraw[:,3]+0.1)   #从MIMICraw 数据集中选取第 4 列（索引为 3）的所有数据，将所有元素加0.1为了避免对数运算中出现取零的对数，处理后将shujv取对数
    c,cmu,csigma = my_zscore(MIMICraw[:,4:36])   #从 MIMICraw 数据集中选取第 5 列（索引为 4）到第 36 列的所有行数据，用my_zscore函数最选取的数据进行标准化处理，将数据转化为均值为0，标准差为1的标准正态分布；函数返回三个值：c标准化后的数据/cmu原始数据的均值/csigma原始数据的标准差
    d,dmu,dsigma = my_zscore(np.log(0.1+MIMICraw[:,36:47]))   #从数据集中第37列到47列的所有行数取出，将每个元素加上0.1后取对数，然后调用my_zscore函数标准化处理，返回三个值标准化后的数据 d、原始数据的均值 dmu 和标准差 dsigma
    

    ####################### Main LOOP ###########################    主循环
    bestpol = 0 
    
    for modl in range(nr_reps):  # MAIN LOOP OVER ALL MODELS 所有模型遍历500个
        N=icuuniqueids.size # total number of rows to choose from  #获取 icuuniqueids 的元素数量，即患者 ID 的总数
        grp=np.floor(ncv*np.random.rand(N,1)+1);  #list of 1 to 5 (20% of the data in each grp) -- this means that train/test MIMIC split are DIFFERENT in all the 500 models  生成一个长度为 N 的数组 grp，其中每个元素是 1 到 ncv 之间的随机整数，用于将数据划分为 ncv 个组，这里 ncv 表示交叉验证的折数
        crossval=1;  #不懂，这个变量前面没有啊，这是干嘛的？交叉验证的折，意会一下，就是那个验证集的占比1（：4）
        trainidx=icuuniqueids[np.where(grp!=crossval)[0]]  #根据 grp 数组将患者 ID 划分为训练集和测试集
        testidx=icuuniqueids[np.where(grp==crossval)[0]]
        train=np.isin(icustayidlist,trainidx)  #生成布尔数组 train 和 test，用于标记每个样本是否属于训练集或测试集
        test=np.isin(icustayidlist,testidx)
        X=MIMICzs[train,:] #根据 train 数组将 MIMICzs 数据集划分为训练集 X 和测试集 Xtestmimic
        Xtestmimic=MIMICzs[~train,:]
        blocs=reformat5[train,0]  #根据 train 数组将 reformat5 数据集的第一列划分为训练集 blocs 和测试集 bloctestmimic
        bloctestmimic=reformat5[~train,0]
        ptid=reformat5[train,1]  #根据 train 数组将 reformat5 数据集的第二列划分为训练集 ptid 和测试集 ptidtestmimic
        ptidtestmimic=reformat5[~train,1] 
        outcome=9 #   HOSP _ MORTALITY = 7 / 90d MORTA = 9 设置要预测的结果列索引为 9，90天死亡率
        Y90=reformat5[train,outcome];   #从训练集中提取要预测的结果列数据


        print('########################   MODEL NUMBER : ',modl)   #打印当前模型的编号和当前时间，方便监控模型训练的进度
        print(datetime.datetime.now())

        #######   find best clustering solution (lowest intracluster variability)  ####################  寻找最佳聚类解决方案（集群内变异性最低）
        print('####  CLUSTERING  ####') # BY SAMPLING 输出“聚类”
        N=X.shape[0] #total number of rows to choose from   #获取训练集 X 的样本数量
        sampl=X[np.where(np.floor(np.random.rand(N,1)+prop))[0],:]  #从训练集 X 中随机采样一部分数据作为聚类的输入，采样比例由 prop 决定

        C = kmeans_with_multiple_runs(ncl,10000,nclustering,sampl)  #调用 kmeans_with_multiple_runs 函数对采样数据进行多次 K-Means 聚类，返回最优的聚类模型 C
        idx = C.predict(X)  #使用最优的聚类模型 C 对整个训练集 X 进行聚类预测，得到每个样本所属的聚类编号

        ############################## CREATE ACTIONS  ########################   创建动作
        print('####  CREATE ACTIONS  ####') 

        nact=nra*nra #动作总数？

        iol=MIMICtable.columns.get_loc('input_4hourly')  #获取 MIMICtable 数据集中 input_4hourly 列的索引，可能表示每四小时静脉输液量？
        vcl=MIMICtable.columns.get_loc('max_dose_vaso')  #获取 MIMICtable 数据集中 max_dose_vaso 列的索引，血管活性药物最大剂量？

        a= reformat5[:,iol].copy()                   # IV fluid 复制 reformat5 数据集中 input_4hourly 列的数据到 a 中
        a= rankdata(a[a>0])/a[a>0].shape[0]   # excludes zero fluid (will be action 1)  rankdata(a[a > 0]) 会对 a 中所有大于 0 的元素进行排名，并返回一个包含排名的数组；a[a > 0]是一个布尔索引操作，它从数组 a 中筛选出所有大于 0 的元素，a[a > 0].shape[0] 表示 a 中大于 0 的元素的数量；将排名数组除以这个数量，就实现了对排名的归一化处理，将排名值缩放到 [0, 1] 区间内；其中0值作为一个输液量被设置为动作1

        iof=np.floor((a+0.2499999999)*4)  #converts iv volume in 4 actions  #将数组a中数据处理（+0.249999之后再乘4）后使用floor 函数对前面计算得到的数组进行向下取整操作将最终的结果赋值给变量 iof。操作最后将输液量划分为4个不同类别或者等级，定为4个动作

        a= reformat5[:,iol].copy()  #再次复制 reformat5 数据集中 input_4hourly 列的数据到 a 中？不会影响吗
        a= np.where(a>0)[0]  # location of non-zero fluid in big matrix  #找出非零数据的索引

        io=np.ones((reformat5.shape[0],1))  # array of ones, by default   创建一个元组，这里表示行数为 reformat5 的行数，列数为 1；使用 numpy 的 ones 函数创建一个指定形状的数组，数组中的所有元素都初始化为 1 
        io[a]=(iof+1).reshape(-1,1)   # where more than zero fluid given: save actual action  此语句是为了在静脉输液量大于 0 的位置，保存实际的动作编号； iof是一个保存动作编号的数组，a是非零输液量数据的索引，【(iof + 1).reshape(-1, 1)：将 iof + 1 数组重新调整形状为列向量，-1 表示该维度的大小由数组的总元素数和其他维度的大小自动计算得出？不懂】
        io = io.ravel()  #使用 numpy 的 ravel 函数将 io 数组展平为一维数组。ravel 函数会返回一个一维数组，包含 io 数组中的所有元素，并重新赋值给io
        #以上 io 数组就记录了每个样本对应的动作编号，其中默认值为 1，静脉输液量大于 0 的位置保存了实际的动作编号


        vc=reformat5[:,vcl].copy()  #将血管活性药物相关的列取出创建副本，赋值给vc
        vcr= rankdata(vc[vc!=0])/vc[vc!=0].size    #使用布尔索引，从 vc 数组中筛选出所有非零元素，得到一个新的数组，将数组除以所有非零元素个数实现归一化操作，归一化数据存储在vcr中
        vcr=np.floor((vcr+0.249999999999)*4)  # converts to 4 bins  取整数，划分四个区间
        vcr[vcr==0]=1  #将vcr中所有为0的元素替换为1
        vc[vc!=0]=vcr+1  #将vc中所有非零元素替换为vcr+1，使得编号从2-5
        vc[vc==0]=1    #将0数据换成1，相当于分成4个动作

        ma1 = np.array([np.median(reformat5[io==1,iol]),np.median(reformat5[io==2,iol]),np.median(reformat5[io==3,iol]), np.median(reformat5[io==4,iol]),np.median(reformat5[io==5,iol])]) # median dose of drug in all bins
        #此代码计算了 reformat5 数据集中 iol 列在不同 io 值条件下的中位数，并将这些中位数存储在一个 numpy 数组 ma1 中；ma1 存储的是所有分组下药物的中位剂量
        ma2 = np.array([np.median(reformat5[vc==1,vcl]),np.median(reformat5[vc==2,vcl]),np.median(reformat5[vc==3,vcl]), np.median(reformat5[vc==4,vcl]),np.median(reformat5[vc==5,vcl])])
        #此代码计算了 reformat5 数据集中 vcl 列在不同 vc 值条件下的中位数，并将这些中位数存储在一个 numpy 数组 ma2 中

        med = np.concatenate([io.reshape(-1,1),vc.reshape(-1,1)],axis=1)  #将io和vc两个一维数组进行合并，形成二维数组med；reshape 函数用于改变数组的形状，-1 表示该维度的大小由数组的总元素数和其他维度的大小自动计算得出，1 表示将一维数组转换为列向量；np.concatenate 函数用于连接数组。axis=1 表示沿着列方向（即第二个轴）进行连接，形成一个二维数组 med，其中 med 的每一行包含了 io 和 vc 对应的两个动作编号
        uniqueValues,actionbloc = np.unique(med,axis=0,return_inverse=True)   #np.unique 函数用于找出数组中的唯一元素。axis=0 表示沿着行方向（即第一个轴）进行操作，即找出 med 数组中的唯一行。return_inverse=True 表示除了返回唯一行的数组外，还返回一个数组，该数组记录了 med 中每一行在唯一行数组中的索引
        #uniqueValues：返回的唯一行的数组，即 med 数组中所有不同的行组成的数组；actionbloc：返回的索引数组，其长度与 med 的行数相同，actionbloc 中的每个元素表示 med 中对应行在 uniqueValues 数组中的索引
        #这两句代码先将 io 和 vc 两个一维数组转换为列向量并按列合并成二维数组 med，然后找出 med 数组中的唯一行，并获取 med 中每一行对应在唯一行数组中的索引

        actionbloctrain=actionbloc[train]  #actionbloc[train]：这是一个布尔索引操作。它会依据 train 数组中的布尔值，从 actionbloc 数组里筛选出对应位置为 True 的元素；也就是说，它会提取出 actionbloc 中属于训练集的数据

        ma2Values = ma2[uniqueValues[:,1].astype('int64')-1].reshape(-1,1)
        ma1Values = ma1[uniqueValues[:,0].astype('int64')-1].reshape(-1,1)
        #根据 uniqueValues 数组中的索引信息，从 ma1 和 ma2 数组中提取对应于不同动作编号组合的中位值，并将这些中位值分别存储在 ma1Values 和 ma2Values 列向量中

        uniqueValuesdose = np.concatenate([ma2Values,ma1Values],axis=1) # median dose of each bin for all 25 actions 
        #将分别存储血管活性药物剂量中位值和静脉输液量中位值的 ma2Values 和 ma1Values 列向量按列拼接，得到一个新的二维数组 uniqueValuesdose，该数组记录了所有动作组合下的中位剂量信息

        ####################################################################################################################################
        print('####  CREATE QLDATA3  ####')   #开始创建qldata3数据

        r=np.array([100, -100]).reshape(1,-1)   #创建一个形状为 (1, 2) 的二维数组 r，其中包含两个奖励值 100 和 -100
        r2=r*(2*(1-Y90.reshape(-1,1))-1)   #是一个代表患者结局（如 90 天死亡率）的数组，Y90.reshape(-1, 1) 将其转换为列向量，然后对结局数据进行线性转换，最后将r与变换后结果相乘，得到r2，是根据结局计算得到的奖励数组
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)
        qldata=np.concatenate([blocs.reshape(-1,1), idx.reshape(-1,1), actionbloctrain.reshape(-1,1), Y90.reshape(-1,1), r2],axis=1)  # contains bloc / state / action / outcome&reward   blocs 可能代表患者的某个分组信息，idx 是聚类状态索引，actionbloctrain 是动作索引，Y90 是结局信息，r2 是奖励信息；把这些数组都转换为列向量后，使用 np.concatenate 函数按列拼接成 qldata 数组。qldata 包含了患者的分组、状态、动作、结局和奖励等信息  
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),4)) #创建一个全零的二维数组 qldata3，其行数是 qldata 行数的 1.2 倍（向下取整），列数为 4。这样做是为了预留足够的空间来存储处理后的数据
        c=-1 
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 

        for i in range(qldata.shape[0]-1):  #遍历qladata数组（除了最后一行）
            c=c+1
            qldata3[c,:]=qldata[i,0:4]  #将 qldata 第 i 行的前 4 列数据复制到 qldata3 的第 c 行
            if(qldata[i+1,0]==1): #end of trace for this patient  #如果 qldata 下一行的第 1 列元素为 1，意味着当前患者的数据记录结束
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,4]])  #在 qldata3 中添加一行新数据，包含更新后的分组信息、吸收状态编号、动作设为 -1 以及奖励信息。

        qldata3=qldata3[:c+1,:]  #根据计数器 c 的值，截取 qldata3 数组，只保留实际填充数据的行

        #上面这一段是，在干嘛啊.../挠头
        # ###################################################################################################################################
        print("####  CREATE TRANSITION MATRIX T(S'',S,A) ####")   #程序准备开始创建状态矩阵
        transitionr=np.zeros((ncl+2,ncl+2,nact))  #this is T(S',S,A)  创建状态转移函数
        sums0a0=np.zeros((ncl+2,nact)) # 辅助矩阵：统计每个状态 S 执行动作 A 的总次数

        for i in range(qldata3.shape[0]-1):    #遍历数据序列
            if (qldata3[i+1,0]!=1) : # if we are not in the last state for this patient = if there is a transition to make!如果不是终止状态，则继续
                S0=int(qldata3[i,1]) 
                S1=int(qldata3[i+1,1])
                acid= int(qldata3[i,2])  #定义当前状态下一状态以及动作
                transitionr[S1,S0,acid]=transitionr[S1,S0,acid]+1 #记录转移次数；从状态 S0 执行动作 acid 后转移到状态 S1 的次数
                sums0a0[S0,acid]=sums0a0[S0,acid]+1  #记录在S0下执行动作 acid 的总次数

        sums0a0[sums0a0<=transthres]=0  #delete rare transitions (those seen less than 5 times = bottom 50%!!)  某些状态下动作出现的次数极少，可能是偶然发生，也可能是不准确数据；为了让让得到的状态转移数据更能反映真实的游戏情况，代码会把这些出现次数很少的转移情况都删掉


        for i in range(ncl+2): 
            for j in range(nact): #两层循环遍历所有的状态动作对
                if sums0a0[i,j]==0: 
                    transitionr[:,i,j]=0; #如果在某状态下从未做过动作j，那么相对应的状态转移次数也为0
                else:
                    transitionr[:,i,j]=transitionr[:,i,j]/sums0a0[i,j] #如果不为0，那么将 transitionr[:, i, j] 中的每个元素除以 sums0a0[i, j]，将次数转换成概率


        transitionr[np.isnan(transitionr)]=0  #replace NANs with zeros
        transitionr[np.isinf(transitionr)]=0  #replace NANs with zeros处理异常值，将NaN和inf值换成0

        physpol=sums0a0/np.sum(sums0a0, axis=1).reshape(-1,1)    #physicians policy: what action was chosen in each state 将 sums0a0 数组中的每个元素除以对应状态下执行所有动作的总次数。这样得到的 physpol 矩阵中的元素就表示在每个状态下执行某个动作的概率


        print("####  CREATE TRANSITION MATRIX T(S,S'',A)  ####")  #创建矩阵T(S,S'',A) 

        transitionr2=np.zeros((ncl+2,ncl+2,nact))  # this is T(S,S',A) 创建新矩阵
        sums0a0=np.zeros((ncl+2,nact))

        for i in range(qldata3.shape[0]-1) : 
            if (qldata3[i+1,0]!=1) : # if we are not in the last state for this patient = if there is a transition to make!
                S0=int(qldata3[i,1])
                S1=int(qldata3[i+1,1])
                acid= int(qldata3[i,2]) 
                transitionr2[S0,S1,acid]=transitionr2[S0,S1,acid]+1;  
                sums0a0[S0,acid]=sums0a0[S0,acid]+1

        sums0a0[sums0a0<=transthres]=0;  #delete rare transitions (those seen less than 5 times = bottom 50%!!) IQR = 2-17

        for i in range(ncl+2): 
            for j in range(nact): 
                if sums0a0[i,j]==0:
                    transitionr2[i,:,j]=0 
                else: 
                    transitionr2[i,:,j]=transitionr2[i,:,j]/sums0a0[i,j]

        transitionr2[np.isnan(transitionr2)]=0 #replace NANs with zeros
        transitionr2[np.isinf(transitionr2)]=0 # replace infs with zeros  
        #基本与上一个矩阵过程一致，在实际问题中，可能需要从不同方向分析状态转移情况，所以创建两个矩阵

        print('####  CREATE REWARD MATRIX  R(S,A) ####')  #创建奖励矩阵R
        # CF sutton& barto bottom 1998 page 106. i compute R(S,A) from R(S'SA) and T(S'SA)
        r3=np.zeros((ncl+2,ncl+2,nact))  #使用 numpy 的 zeros 函数创建一个全零的三维数组 r3，其形状为 (ncl + 2, ncl + 2, nact)
        r3[ncl,:,:]=-100  #终极状态（死亡）负奖励
        r3[ncl+1,:,:]=100  
        R=sum(transitionr*r3) #transitionr * r3 是将状态转移矩阵 transitionr 和三维奖励矩阵 r3 对应元素相乘，得到每个状态转移情况下的期望奖励；sum(...) 函数会对相乘后的结果沿着第一个维度（即状态 S' 的维度）进行求和，得到在每个状态 S 下执行每个动作 A 的期望奖励，最终得到二维的奖励矩阵 R(S, A)
        R=np.squeeze(R)   #remove 1 unused dimension使用 numpy 的 squeeze 函数去除 R 矩阵中长度为 1 的维度；由于求和操作可能会导致出现长度为 1 的维度，这里将其去除，使矩阵的形状更加简洁（这个...没太明白为什么会有长度为1的维度，需要具体数据算一下看）


        print('####  POLICY ITERATION   ####')  #策略迭代


        pi = PolicyIteration_with_Q(transitionr2, R, gamma, np.ones((ncl+2,1))) #PolicyIteration_with_Q 是一个自定义的类，用于实现基于 Q 值的策略迭代算法；类中元素如其所写，其中np.ones((ncl + 2, 1)) 是一个初始的策略向量，其中 ncl + 2 表示状态的数量，初始时每个状态对应的策略值都设为 1
        Qon = np.transpose(pi.run()) #pi.run() 调用 PolicyIteration_with_Q 类中的 run 方法，该方法执行策略迭代算法，通过不断地评估和改进策略，最终得到每个状态 - 动作对的 Q 值；np.transpose(...) 对 pi.run() 的结果进行转置操作，将得到的 Q 值矩阵转换为合适的形状
        OptimalAction=np.argmax(Qon,axis=1).reshape(-1,1)  #deterministic np.argmax(Qon, axis = 1) 沿着 Qon 矩阵的第一个维度（即状态维度）找到每个状态下 Q 值最大的动作的索引；因为 Q 值表示执行某个动作后能获得的期望累积奖励，所以 Q 值最大的动作就是在该状态下的最优动作；OptimalAction 矩阵存储了每个状态下的最优动作，是一个确定性的策略
        OA[:,modl]=OptimalAction.ravel() #save optimal actions OA 是一个二维数组，用于保存不同模型或不同迭代步骤下的最优动作；modl 表示当前模型或迭代步骤的索引；OptimalAction.ravel() 将 OptimalAction 矩阵展平为一维数组，然后将其赋值给 OA 数组的第 modl 列，从而保存当前模型下每个状态的最优动作


        print('####  OFF-POLICY EVALUATION - MIMIC TRAIN SET ####')  #程序即将开始在 MIMIC 训练集上进行离线策略评估
        # create new version of QLDATA3创建奖励相关数组

        r=np.array([100, -100]).reshape(1,-1)  #创建一个包含两个元素 [100, -100] 的一维数组，然后将其重塑为形状为 (1, 2) 的二维数组
        r2=r*(2*(1-Y90.reshape(-1,1))-1) #90天死亡率的值与 r 数组相乘，得到每个样本对应的奖励 r2
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)
        qldata=np.concatenate([blocs.reshape(-1,1), idx.reshape(-1,1), actionbloctrain.reshape(-1,1), Y90.reshape(-1,1),np.zeros((idx.size,1)), r2[:,0].reshape(-1,1), ptid.reshape(-1,1) ],axis=1)   # contains bloc / state / action / outcome&reward     
        #qldata 数组按列拼接多个数组，包含了blocs、状态信息（idx）、动作信息（actionbloctrain）、结果信息（Y90）、一个占位的零列、奖励信息（r2）以及患者 ID 信息（ptid）
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),8))   #创建一个全零的二维数组 qldata3，其行数为 qldata 数组行数的 1.2 倍（向下取整并转换为 int64 类型），列数为 8；可能是为了预留一些额外的空间？
        c=-1
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 包含两个元素的数组 abss，ncl + 1 和 ncl 代表吸收状态的编号；吸收状态是指进入后就不能再转移到其他状态的特殊状态，如死亡或康复出院等

        for i in range(qldata.shape[0]-1): #使用 for 循环遍历 qldata 数组的每一行（除了最后一行）  本循环主要是对 qldata 进行处理，将其转换为 qldata3 ，并在遇到患者数据记录结束时添加吸收状态相关信息，最后截取 qldata3 中有效数据部分
            c=c+1  #初始c=-1
            qldata3[c,:]=qldata[i,[0,1,2,4,6,6,6,6]] #从 qldata 的第 i 行中选取第 0、1、2、4、6 列（这里第 6 列被重复选取）的数据；然后将这些数据赋值给 qldata3 的第 c 行
            if(qldata[i+1,0]==1): #end of trace for this patient 检查 qldata 中下一行（即 i + 1 行）的第 0 列元素是否等于 1；如果等于 1，说明当前患者的数据记录结束
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,5],0,0,-1,qldata[i,6]]) #当检测到患者数据记录结束时，c 再次加 1，然后在 qldata3 的第 c 行添加一条新的数据记录

        qldata3=qldata3[:c+1,:]  #c 记录了 qldata3 中实际填充的最后一行的索引，通过 qldata3[:c + 1, :] 截取 qldata3 中从第 0 行到第 c 行的所有列，去除之前初始化时预留但未使用的多余行，得到最终有效的 qldata3 数组

        #  add pi(s,a) and b(s,a)  pi是行为策略，供参考；b为目标策略（具体可参照动态决策AI笔记，后续会继续更新）
        p=0.01 #softening policies 软策略
        softpi=physpol.copy() # behavior policy = clinicians' #复制 physpol 矩阵得到 softpi，作为软化后的行为策略矩阵
        for i in range(ncl): #遍历每个状态
            ii=softpi[i,:]==0    #找出概率为 0 的动作
            z=p/sum(ii)    #sum(ii) 是状态 i 下概率为 0 的动作数量，将 p 平均分配给这些动作
            nz=p/sum(~ii)    #sum(~ii) 是状态 i 下概率不为 0 的动作数量，将 p 平均从这些动作的概率中减去(为什么是平均？)
            softpi[i,ii]=z;   #将状态 i 下概率为 0 的动作的概率更新为 z
            softpi[i,~ii]=softpi[i,~ii]-nz; #将状态 i 下概率不为 0 的动作的概率减去 nz

        softb=np.abs(np.zeros((ncl+2,nact))-p/24) #"optimal" policy = target policy = evaluation policy np.zeros((ncl + 2, nact)) 创建一个形状为 (ncl + 2, nact) 的全零矩阵；softb 是软化后的目标策略矩阵，在离线策略评估中，目标策略是我们想要评估的策略，通常是一个理论上的最优策略

        for i in range(ncl): 
            softb[i,OptimalAction[i]]=1-p #对于每个状态 i，将目标策略矩阵 softb 中该状态 i 下的最优动作对应的概率设置为 1 - p


        for i in range(qldata3.shape[0]):  # adding the probas of policies to qldata3遍历 qldata3 矩阵的每一行
            if qldata3[i,1]<ncl :  #筛选出状态编号小于 ncl 的行
                qldata3[i,4]=softpi[int(qldata3[i,1]),int(qldata3[i,2])] #qldata3[i, 4] 存储行为策略 softpi 在当前状态（qldata3[i, 1]）下执行当前动作（qldata3[i, 2]）的概率
                qldata3[i,5]=softb[int(qldata3[i,1]),int(qldata3[i,2])]  #qldata3[i, 5] 存储目标策略 softb 在当前状态下执行当前动作的概率（准备计算采样比？）
                qldata3[i,6]=OptimalAction[int(qldata3[i,1])]  #optimal action qldata3[i, 6] 存储当前状态下的最优动作

        qldata3train=qldata3.copy()  #复制 qldata3 矩阵得到 qldata3train，这样做可能是为了后续对训练数据进行操作


        bootql,bootwis = offpolicy_multiple_eval_010518(qldata3,physpol, 0.99,1,6,750)  #offpolicy_multiple_eval_010518 是一个自定义的函数，用于进行离线策略评估（其他文件中或有定义）；函数返回两个结果：bootql 和 bootwis，它们可能分别代表某种评估指标

        recqvi[modl,0]=modl  #recqvi 是一个用于记录评估结果的矩阵，modl 表示当前模型或迭代步骤的索引
        recqvi[modl,3]=np.nanmean(bootql) 
        recqvi[modl,4]=mquantiles(bootql,0.99, alphap=0.5, betap=0.5)[0]
        recqvi[modl,5]=np.nanmean(bootwis) # we want this as high as possible #recqvi[modl, 5] 记录 bootwis 的均值，通常希望这个值尽可能大
        recqvi[modl,6]=mquantiles(bootwis,0.05, alphap=0.5, betap=0.5)[0] #we want this as high as possible  recqvi[modl, 6] 记录 bootwis 的 5% 分位数，同样希望这个值尽可能大


        # testing on MIMIC-test 离线策略评估（Off - Policy Evaluation）的准备工作
        print('####  OFF-POLICY EVALUATION - MIMIC TEST SET ####')  

        # create new version of QLDATA3 with MIMIC TEST samples
        #预测测试集样本的状态
        idxtest = C.predict(Xtestmimic) #C 应该是一个已经训练好的分类器或模型；Xtestmimic 是 MIMIC 测试集的特征数据；通过调用 C.predict 方法，对测试集样本进行预测，得到每个样本所属的状态编号，存储在 idxtest 中
        #记录测试集样本的状态信息
        idxs[test,modl]=idxtest.ravel()  #important: record state membership of test cohortidxtest.ravel() 将 idxtest 数组展平为一维数组，然后将其赋值给 idxs 数组的对应位置，用于记录测试集样本的状态归属
        #提取测试集的动作和结果信息
        actionbloctest=actionbloc[~train] #actionbloc 是包含所有样本动作信息的数组，train 是一个布尔数组，用于标记训练集样本；~train 表示取反，即选择非训练集（测试集）的样本
        Y90test=reformat5[~train,outcome] #reformat5 是包含所有样本相关信息的数组，outcome 表示结果信息的列索引；Y90test 提取了测试集样本的结果信息

        r=np.array([100, -100]).reshape(1,-1)
        r2=r*(2*(1-Y90test.reshape(-1,1))-1)
        # because idx and actionbloctrain are index, it's equal to (Matlab's original value -1)

        qldata=np.concatenate([bloctestmimic.reshape(-1,1), idxtest.reshape(-1,1), actionbloctest.reshape(-1,1), Y90test.reshape(-1,1),np.zeros((idxtest.size,1)), r2[:,0].reshape(-1,1), ptidtestmimic.reshape(-1,1) ],axis=1)   # contains bloc / state / action / outcome&reward     
        # 0 = died in Python, 1 = died in Matlab 
        qldata3=np.zeros((np.floor(qldata.shape[0]*1.2).astype('int64'),8))
        c=-1
        abss=np.array([ncl+1, ncl]) #absorbing states numbers # 751, 750 
#上面这段是对 MIMIC 测试集数据进行处理，通过预测样本状态、提取动作和结果信息、计算奖励等操作，将测试集数据整理成 qldata 数组，并初始化 qldata3 数组，为后续在测试集上进行离线策略评估做好数据准备
        for i in range(qldata.shape[0]-1):
            c=c+1
            qldata3[c,:]=qldata[i,[0,1,2,4,6,6,6,6]] #复制部分列数据：将 qldata 数组第 i 行的第 0、1、2、4、6 列（第 6 列重复 3 次）的数据复制到 qldata3 数组的第 c 行（为什么是这几行？）
            if(qldata[i+1,0]==1): #end of trace for this patient
                c=c+1     
                qldata3[c,:]=np.array([qldata[i,0]+1, abss[int(qldata[i,3])], -1, qldata[i,5],0,0,-1,qldata[i,6]]) #与上面类似

        qldata3=qldata3[:c+1,:]

        #  add pi(s,a) and b(s,a)
        p=0.01 # small correction factor #softening policies 
        softpi=physpol.copy() # behavior policy = clinicians' 
        for i in range(ncl): 
            ii=softpi[i,:]==0    
            z=p/sum(ii)    
            nz=p/sum(~ii)    
            softpi[i,ii]=z;   
            softpi[i,~ii]=softpi[i,~ii]-nz;

        softb=np.abs(np.zeros((ncl+2,nact))-p/24) #"optimal" policy = target policy = evaluation policy #与上pi与b策略的软化类似

        for i in range(ncl): 
            softb[i,OptimalAction[i]]=1-p

        for i in range(qldata3.shape[0]):  # adding the probas of policies to qldata3
            if qldata3[i,1]<ncl :
                qldata3[i,4]=softpi[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,5]=softb[int(qldata3[i,1]),int(qldata3[i,2])]
                qldata3[i,6]=OptimalAction[int(qldata3[i,1])]  #optimal action

        qldata3test=qldata3.copy() 

        bootmimictestql,bootmimictestwis = offpolicy_multiple_eval_010518(qldata3,physpol, 0.99,1,6,2000)

        recqvi[modl,18]=mquantiles(bootmimictestql,0.95, alphap=0.5, betap=0.5)[0] #PHYSICIANS' 95% UB
        recqvi[modl,19]=np.nanmean(bootmimictestql)
        recqvi[modl,20]=mquantiles(bootmimictestql,0.99, alphap=0.5, betap=0.5)[0]
        recqvi[modl,21]=np.nanmean(bootmimictestwis) 
        recqvi[modl,22]=mquantiles(bootmimictestwis,0.01, alphap=0.5, betap=0.5)[0] 
        recqvi[modl,23]=mquantiles(bootmimictestwis,0.05, alphap=0.5, betap=0.5)[0] #AI 95% LB, we want this as high as possible
#与上类似 这段目的是完善目标策略矩阵，为测试集数据添加策略概率和最优动作信息，在测试集上进行离线策略评估，并将评估结果记录下来，为后续的策略分析和比较提供数据支持


#下面这段代码主要完成了模型筛选、保存和结果记录的工作，具体来说，它会根据模型在测试集上的评估结果决定是否进行后续测试、保存表现良好的模型，最后将相关数据保存为 pickle 文件和 CSV 文件
        if recqvi[modl,23] > 40: #saves time if policy is not good on MIMIC test: skips to next modelrecqvi[modl,23] 存储的是模型在 MIMIC 测试集上评估指标的 95% 下界，如果该值大于 40，说明模型在 MIMIC 测试集上表现不佳，为节省时间，跳过 eICU 测试
            print('########################## eICU TEST SET #############################')
            # eICU part was not implemented 



        # eICU testing was not included 
        if recqvi[modl,23]>0 : #  & recqvi(modl,14)>0   # if 95% LB is >0 : save the model (otherwise it's pointless) 如果 recqvi[modl,23] 大于 0，说明模型在 MIMIC 测试集上的 95% 下界为正，认为该模型表现良好，保留
            print('####   GOOD MODEL FOUND - SAVING IT   ####' ) 

            # best pol 
            if(bestpol < recqvi[modl,23]): 
                print('Best policy was replaced => 95% LB is ',recqvi[modl,23])
                bestpol = recqvi[modl,23]  #进一步比较当前模型的 95% 下界与之前记录的最优策略的 95% 下界（bestpol），如果当前模型更优，则更新 bestpol 的值，并打印替换信息
                
                # save to pickle 
                with open('bestpol.pkl', 'wb') as file:
                    pickle.dump(modl,file)
                    pickle.dump(Qon,file)
                    pickle.dump(physpol,file)
                    pickle.dump(transitionr,file) 
                    pickle.dump(transitionr2,file)
                    pickle.dump(R,file)
                    pickle.dump(C,file)
                    pickle.dump(train,file)
                    pickle.dump(qldata3train,file)
                    pickle.dump(qldata3test,file)   #使用 pickle 模块将当前模型的相关数据（如模型索引 modl、Q 值矩阵 Qon、医生策略矩阵 physpol、状态转移矩阵 transitionr 和 transitionr2、奖励矩阵 R、分类器 C、训练集标记 train、训练集数据 qldata3train 和测试集数据 qldata3test）保存到 bestpol.pkl 文件中


    recqvi=recqvi[:modl+1,:]  #截取 recqvi 矩阵从第 0 行到第 modl 行的所有列，去除未使用的多余行，得到最终有效的 recqvi 矩阵

    # save to pickle for visualization 
    with open('step_5_start.pkl', 'wb') as file:
        pickle.dump(MIMICzs,file)
        pickle.dump(actionbloc,file)
        pickle.dump(reformat5,file)
        pickle.dump(recqvi,file)  #使用 pickle 模块将 MIMICzs、actionbloc、reformat5 和 recqvi 保存到 step_5_start.pkl 文件中

    # save recqvi in csv format 
    np.savetxt('recqvi.csv',recqvi,delimiter=',')  #使用 numpy 的 savetxt 函数将 recqvi 矩阵保存为 recqvi.csv 文件



# In[ ]:




